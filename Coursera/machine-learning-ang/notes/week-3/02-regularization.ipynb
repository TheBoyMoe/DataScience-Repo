{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Overfitting\n",
    "\n",
    "Machine learning models need to generalize well to new examples that the model has not seen in practice. In this module, we introduce regularization, which helps prevent models from overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem of Overfitting\n",
    "\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-19.png)\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-20.png)\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-21.png)\n",
    "\n",
    "Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. At the other extreme, overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.\n",
    "\n",
    "This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:\n",
    "\n",
    "1. Reduce the number of features:\n",
    "\n",
    "- Manually select which features to keep.\n",
    "- Use a model selection algorithm (studied later in the course).\n",
    "\n",
    "2. Regularization\n",
    "\n",
    "- Keep all the features, but reduce the magnitude of parameters Î¸_j\n",
    "- Regularization works well when we have a lot of slightly useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-22.png)\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-23.png)\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-25.png)\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n",
    "We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pink line, is less likely to overfit than the non-regularized function represented by the blue line:\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-27.png)\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-28.png)\n",
    "\n",
    "![Logistic Regression](imgs/logistic-regression-29.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
