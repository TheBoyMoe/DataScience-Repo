{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Involves four stages:\n",
    "\n",
    "1. Model Specification\n",
    "\n",
    "2. Compile the model\n",
    "\n",
    "3. Fit the Model\n",
    "\n",
    "4. Make the Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### WILL NOT RUN LOCALLY AS GPU REQ'D ##########\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# target -> 'wages_per_hour'\n",
    "wages = pd.read_csv('./data/wages.txt')\n",
    "print(wages.shape)\n",
    "wages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictors into numpy array\n",
    "predictors = wages.drop('wage_per_hour', axis=1).as_matrix()\n",
    "print(type(predictors))\n",
    "print(predictors.shape)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = wages['wage_per_hour'].as_matrix()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Neural Net - two hidden layers and one output layer\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "# Use the .add() method on model to add a Dense layer.\n",
    "# Add 50 units, specify activation='relu', and the input_shape parameter to be \n",
    "# the tuple (n_cols,) which means it has n_cols items in each row of data, and any \n",
    "# number of rows of data are acceptable as inputs.\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "# Add another Dense layer. This should have 32 units and a 'relu' activation.\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "# which is a Dense layer with a single node, NO activation function\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile the model, we need to specify the **optimizer** and **loss function** to use. the **Adam optimizer** is generally excellent choice for optimizer, we'll use **mean-squared error** as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_was_missing</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age  sibsp  parch     fare  male  age_was_missing  \\\n",
       "0         0       3  22.0      1      0   7.2500     1            False   \n",
       "1         1       1  38.0      1      0  71.2833     0            False   \n",
       "2         1       3  26.0      0      0   7.9250     0            False   \n",
       "3         1       1  35.0      1      0  53.1000     0            False   \n",
       "4         0       3  35.0      0      0   8.0500     1            False   \n",
       "\n",
       "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "0                        0                         0   \n",
       "1                        1                         0   \n",
       "2                        0                         0   \n",
       "3                        0                         0   \n",
       "4                        0                         0   \n",
       "\n",
       "   embarked_from_southampton  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titanic = pd.read_csv('./data/titanic.csv')\n",
    "print(titanic.shape)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = titanic.drop('survived', axis=1).as_matrix()\n",
    "\n",
    "# Convert df.survived to a categorical variable using the to_categorical() function\n",
    "target = to_categorical(titanic.survived)\n",
    "\n",
    "n_cols = predictors.shape[1] # 10\n",
    "\n",
    "print(predictors.shape, target.shape, n_cols)\n",
    "print(Setup complete!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "# Add a Dense layer with 32 nodes. Use 'relu' as the activation and (n_cols,) as the input_shape\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "# Add the Dense output layer. Because there are two outcomes, it should have 2 units, and \n",
    "# because it is a classification model, the activation should be 'softmax'.\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "# Compile the model, using 'sgd' as the optimizer, 'categorical_crossentropy' as the loss function, \n",
    "# and metrics=['accuracy'] to see the accuracy (what fraction of predictions were correct) \n",
    "# at the end of each epoch\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_data = np.array(\n",
    "    [\n",
    "        [2, 34.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [2, 31.0, 1, 1, 26.25, 0, False, 0, 0, 1],\n",
    "        [1, 11.0, 1, 2, 120.0, 1, False, 0, 0, 1],\n",
    "        [3, 0.42, 0, 1, 8.5167, 1, False, 1, 0, 0],\n",
    "        [3, 27.0, 0, 0, 6.975, 1, False, 0, 0, 1],\n",
    "        [3, 31.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "        [1, 39.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 0, 7.775, 0, False, 0, 0, 1],\n",
    "        [2, 39.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [1, 33.0, 1, 0, 53.1, 0, False, 0, 0, 1],\n",
    "        [3, 26.0, 0, 0, 7.8875, 1, False, 0, 0, 1],\n",
    "        [3, 39.0, 0, 0, 24.15, 1, False, 0, 0, 1],\n",
    "        [2, 35.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [3, 6.0, 4, 2, 31.275, 0, False, 0, 0, 1],\n",
    "        [3, 30.5, 0, 0, 8.05, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 0, 0, 0.0, 1, True, 0, 0, 1],\n",
    "        [3, 23.0, 0, 0, 7.925, 0, False, 0, 0, 1],\n",
    "        [2, 31.0, 1, 1, 37.0042, 1, False, 1, 0, 0],\n",
    "        [3, 43.0, 0, 0, 6.45, 1, False, 0, 0, 1],\n",
    "        [3, 10.0, 3, 2, 27.9, 1, False, 0, 0, 1],\n",
    "        [1, 52.0, 1, 1, 93.5, 0, False, 0, 0, 1],\n",
    "        [3, 27.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [1, 38.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "        [3, 27.0, 0, 1, 12.475, 0, False, 0, 0, 1],\n",
    "        [3, 2.0, 4, 1, 39.6875, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 6.95, 1, True, 0, 1, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 56.4958, 1, True, 0, 0, 1],\n",
    "        [2, 1.0, 0, 2, 37.0042, 1, False, 1, 0, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 7.75, 1, True, 0, 1, 0],\n",
    "        [1, 62.0, 0, 0, 80.0, 0, False, 0, 0, 0],\n",
    "        [3, 15.0, 1, 0, 14.4542, 0, False, 1, 0, 0],\n",
    "        [2, 0.83, 1, 1, 18.75, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "        [3, 23.0, 0, 0, 7.8542, 1, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 0, 8.3, 1, False, 0, 0, 1],\n",
    "        [1, 39.0, 1, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "        [3, 21.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 8.05, 1, True, 0, 0, 1],\n",
    "        [3, 32.0, 0, 0, 56.4958, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 0, 0, 29.7, 1, True, 1, 0, 0],\n",
    "        [3, 20.0, 0, 0, 7.925, 1, False, 0, 0, 1],\n",
    "        [2, 16.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [1, 30.0, 0, 0, 31.0, 0, False, 1, 0, 0],\n",
    "        [3, 34.5, 0, 0, 6.4375, 1, False, 1, 0, 0],\n",
    "        [3, 17.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "        [3, 42.0, 0, 0, 7.55, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 8, 2, 69.55, 1, True, 0, 0, 1],\n",
    "        [3, 35.0, 0, 0, 7.8958, 1, False, 1, 0, 0],\n",
    "        [2, 28.0, 0, 1, 33.0, 1, False, 0, 0, 1],\n",
    "        [1, 29.69911764705882, 1, 0, 89.1042, 0, True, 1, 0, 0],\n",
    "        [3, 4.0, 4, 2, 31.275, 1, False, 0, 0, 1],\n",
    "        [3, 74.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "        [3, 9.0, 1, 1, 15.2458, 0, False, 1, 0, 0],\n",
    "        [1, 16.0, 0, 1, 39.4, 0, False, 0, 0, 1],\n",
    "        [2, 44.0, 1, 0, 26.0, 0, False, 0, 0, 1],\n",
    "        [3, 18.0, 0, 1, 9.35, 0, False, 0, 0, 1],\n",
    "        [1, 45.0, 1, 1, 164.8667, 0, False, 0, 0, 1],\n",
    "        [1, 51.0, 0, 0, 26.55, 1, False, 0, 0, 1],\n",
    "        [3, 24.0, 0, 3, 19.2583, 0, False, 1, 0, 0],\n",
    "        [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "        [3, 41.0, 2, 0, 14.1083, 1, False, 0, 0, 1],\n",
    "        [2, 21.0, 1, 0, 11.5, 1, False, 0, 0, 1],\n",
    "        [1, 48.0, 0, 0, 25.9292, 0, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 8, 2, 69.55, 0, True, 0, 0, 1],\n",
    "        [2, 24.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [2, 42.0, 0, 0, 13.0, 0, False, 0, 0, 1],\n",
    "        [2, 27.0, 1, 0, 13.8583, 0, False, 1, 0, 0],\n",
    "        [1, 31.0, 0, 0, 50.4958, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 9.5, 1, True, 0, 0, 1],\n",
    "        [3, 4.0, 1, 1, 11.1333, 1, False, 0, 0, 1],\n",
    "        [3, 26.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [1, 47.0, 1, 1, 52.5542, 0, False, 0, 0, 1],\n",
    "        [1, 33.0, 0, 0, 5.0, 1, False, 0, 0, 1],\n",
    "        [3, 47.0, 0, 0, 9.0, 1, False, 0, 0, 1],\n",
    "        [2, 28.0, 1, 0, 24.0, 0, False, 1, 0, 0],\n",
    "        [3, 15.0, 0, 0, 7.225, 0, False, 1, 0, 0],\n",
    "        [3, 20.0, 0, 0, 9.8458, 1, False, 0, 0, 1],\n",
    "        [3, 19.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 0, 0, 7.8958, 1, True, 0, 0, 1],\n",
    "        [1, 56.0, 0, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "        [2, 25.0, 0, 1, 26.0, 0, False, 0, 0, 1],\n",
    "        [3, 33.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "        [3, 22.0, 0, 0, 10.5167, 0, False, 0, 0, 1],\n",
    "        [2, 28.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "        [3, 25.0, 0, 0, 7.05, 1, False, 0, 0, 1],\n",
    "        [3, 39.0, 0, 5, 29.125, 0, False, 0, 1, 0],\n",
    "        [2, 27.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "        [1, 19.0, 0, 0, 30.0, 0, False, 0, 0, 1],\n",
    "        [3, 29.69911764705882, 1, 2, 23.45, 0, True, 0, 0, 1],\n",
    "        [1, 26.0, 0, 0, 30.0, 1, False, 1, 0, 0],\n",
    "        [3, 32.0, 0, 0, 7.75, 1, False, 0, 1, 0]\n",
    "    ]\n",
    ")\n",
    "print(pred_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your predictions using the model's .predict() method on pred_data\n",
    "predictions = model.predict(pred_data)\n",
    "\n",
    "# Use NumPy indexing to find the column corresponding to predicted probabilities\n",
    "# of survival being True. This is the second column (index 1) of predictions. \n",
    "predicted_prob_true = predictions[:, 1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Optimisation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What could prevent a model from showing an improved loss in its first few epochs?\n",
    "\n",
    "A. A learning rate that is either too high or too low, or poor choice of activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try optimizing a model at a very low learning rate, a very high learning rate, and a \"just right\" learning rate. We'll look at the results after running this exercise, remembering that a low value for the loss function is good.\n",
    "\n",
    "We want the optimization to start from scratch every time we change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a nn with two hidden layers for classification, with 10 units in each layer\n",
    "def get_new_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create a list of learning rates to try optimizing with called lr_to_test. \n",
    "# The learning rates in it should be .000001, 0.01, and 1\n",
    "lr_to_test = [0.000001, 0.01, 1.0]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model accuracy on validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use part of the data as a validation set withthe `validation_split` argument. K-folds splits are not generally performed since deep learning generally involves using large data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = get_new_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization through Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = get_new_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create an EarlyStopping object. Stop optimization when the validation loss \n",
    "# hasn't improved for 2 epochs by specifying the patience  parameter of 2.\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model, specifing the number of epochs and the size of the validation split and callbacks. \n",
    "model.fit(predictors, target, epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because optimization will automatically stop when it is no longer helpful, it is okay to specify the maximum number of epochs as `30` rather than using the default of 10 that we've used so far. Here, it seems like the optimization stopped after 7 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models - using more nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a new model called `model_2` which is similar to `model_1`, except it has 100 units in each hidden layer.\n",
    "\n",
    "After we create `model_2`, both models will be fitted, and a graph showing both models loss score at each epoch will be shown. We added the argument `verbose=False` in the fitting commands to print out fewer updates, since you will look at these graphically instead of as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# create & and compile model_1\n",
    "model_1 = get_new_model()\n",
    "model_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create & compile model_2\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NN comparison](./img/nn-comparison.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue model is `model_2`, the red is the original `model_1`. `model_2` has a lower loss value, so it is the better model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models - using more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll try a deeper network (more hidden layers).\n",
    "\n",
    "Once again, you have a baseline model called `model_1` as a starting point. It has 2 hidden layer, with 10 units/nodes each. We can print a summary of that model's structure with `model_1.summary()`. We will create a similar network with 3 hidden layers (still keeping the same number of units in each layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# create and compile model_1\n",
    "model_1 = get_new_model()\n",
    "model_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first, second, and third hidden layers\n",
    "model_2.add(Dense(10, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(10, activation='relu'))\n",
    "model_2.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model 1\n",
    "model_1_training = model_1.fit(\n",
    "    predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(\n",
    "    predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# fit both the models and visualize which one gives better results! \n",
    "# For both models, you should look for the best 'val_loss' and 'val_acc', \n",
    "# which won't be the last epoch for that model.\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn comparison 2](./img/nn-comparison-2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue model is `model_2` and the red is the original, `model_1`. The model with the lower loss value is the better model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
