{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "When fitting a linear regression model, what we're doing is choosing parameters, `k`, that fit the model the best.\n",
    "\n",
    "With Ridge/Lasso regression we choose an `alpha` which produces the best model fit.\n",
    "\n",
    "When fitting a **knn** model, we need to choose the `n_neighbors` which gives the best fit.\n",
    "\n",
    "Logistic regression has a **regularisation** parameter, `C`.\n",
    "\n",
    "Such parameters, `k`, `alpha`, `n_neighbors`, are called **hyper parameters**. They can not be learned be fitting the model. You discover the best value through trial and error:\n",
    "\n",
    "- try a bunch of different htperparameter values\n",
    "- fit them all separately\n",
    "- see how each performs\n",
    "- choose the best performing one\n",
    "\n",
    "This is called **hyperparameter tuning**.\n",
    "\n",
    "When fitting different values of a hyperparameter, it is essential to use **cross-validation** because using **test-train split** alone would risk **overfittting** the hyperparameter to the test set.\n",
    "\n",
    "One technique that is used is **grid search cross-validation**:\n",
    "\n",
    "- we choose a grid of the possible hyperparameter(s)\n",
    "- perform `k-fold` cross-validation for each point in the grid(each hyperparameter or choice of hyperparamters)\n",
    "- choose the hyperparameter that performs the best.\n",
    "\n",
    "This example demonstrates a grid where we're comparing two hyperparameters, `C` and `alpha`.\n",
    "\n",
    "![Grid Search](../imgs/logistic-regression-4.png)\n",
    "\n",
    "We can implement **gscv** in sklearn with the `GridSearchCV` function.  We pass it a dictionary where the keys are the hyperparameter, e.g. `n_neighbors`, or `alpha`, and the values are lists we wish to tune the hyperparameter(s) over. If we specify multiple parameters, all combinations will be tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using hyperparameter tuning to find C paramater in Logistic Regression\n",
    "\n",
    "Like the `alpha` parameter of **lasso** and **ridge** regularization, **logistic regression** has a regularization parameter: `C`. `C` controls the inverse of the regularization strength. A large `C` can lead to an overfit model, while a small `C` can lead to an underfit model.\n",
    "\n",
    "We'll use **GridSearchCV** and **logistic regression** to find the optimal `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (768, 8)\n",
      "<class 'numpy.ndarray'> (768,)\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "df = pd.read_csv('../data/diabetes.csv')\n",
    "X = df.drop('diabetes', axis=1).values\n",
    "y = df.diabetes.values\n",
    "\n",
    "print(type(X), X.shape)\n",
    "print(type(y), y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:  We're NOT performing a test-train split in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 163789.3706954068}\n",
      "Best score is 0.7721354166666666\n"
     ]
    }
   ],
   "source": [
    "# Setup the hyperparameter grid by using c_space as the grid of values to tune C over.\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object with 5-fold cross-validation to tune 'C',\n",
    "# specifying the classifier, parameter distribution and number of folds\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# fit the data, executing the grid search\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the best parameter and best score obtained from GridSearchCV \n",
    "# by accessing the best_params_ and best_score_ attributes of logreg_cv.\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`C` value should be 3.72759, best score of 0.77083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomisedSeach CV\n",
    "\n",
    "GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.\n",
    "\n",
    "We'll use a d new algorithm/model, **Decision Tree**. Just like **k-NN**, **linear regression**, and **logistic regression**, **decision trees** in scikit-learn use the same `api`. There are `.fit()` and `.predict()` methods that you can use in exactly the same way. **Decision trees** have many parameters that can be tuned, such as `max_features`, `max_depth`, and `min_samples_leaf`. This makes it an ideal use case for **RandomizedSearchCV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 8, 'min_samples_leaf': 3}\n",
      "Best score is 0.7395833333333334\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object,  specify the classifier, \n",
    "# parameter distribution, and number of folds to use.\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise results:\n",
    "\n",
    "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_features': 3, 'max_depth': 3, 'min_samples_leaf': 7}\n",
    "Best score is 0.7408854166666666\n",
    "\n",
    "**Note**: RandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used ALL our data in **cross-validation**, and so can not report on how well our model might perform against unseen data. We'll want to split our dataset first of all using `train-test-split` and set aside the test data and only perform the **grid search cross-validation** on the training set to choose the best hyperparameters(tune you hyperparameters) and then use the test set to evaluate how well our model will fair with unseen data(b."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
