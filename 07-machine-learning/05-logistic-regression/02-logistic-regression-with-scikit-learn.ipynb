{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logestic regression wih Scikit-Learn\n",
    "\n",
    "Start by importing `LogesticRegression` from the `sklear.linear_model` module, and creating a `LogesticRegression` object.\n",
    "\n",
    "```py\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = Logesticregression()\n",
    "```\n",
    "\n",
    "We then train (or `fit`) our model using the `.fit()` method, which takes two parameters. The first is a matrix of features, and the second is a matrix of class labels.\n",
    "\n",
    "```py\n",
    "model.fit(features, labels)\n",
    "```\n",
    "\n",
    "When we fit the model with sklearn it will perform gradient descent, repeatedly updating the coefficients of our model in order to minimize the log-loss.\n",
    "\n",
    "Once trained, we can access the `coef_` attribute, which is a vector of the coefficients of each feature, and the `intercept_`, the `b_0` value.\n",
    "\n",
    "We can also predict whether new data points belong to the positive class using the `.predict()` method. It takes a matrix of features as a parameter and returns a vector of labels 1 or 0 for each sample. `sklearn` uses a classification threshold of 0.5.\n",
    "\n",
    "```py\n",
    "model.predict(features)\n",
    "```\n",
    "\n",
    "If we are more interested in the predicted probability of the data samples belonging to the positive class than the actual class, we can use the `.predict_proba()` method. It takes a matrix of features as a parameter and returns a vector of probabilities, ranging from 0 to 1, for each sample.\n",
    "\n",
    "```py\n",
    "model.predict_proba(features)\n",
    "```\n",
    "`sklearn`'s Logistic Regression implementation requires that feature data be normalized since it uses `Regularization`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from exam import hours_studied_scaled, passed_exam, exam_features_scaled_train, exam_features_scaled_test, passed_exam_2_train, passed_exam_2_test, guessed_hours_scaled\n",
    "\n",
    "# Create logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model using hours_studied_scaled as the training features and passed_exam as the training labels.\n",
    "model.fit(hours_studied_scaled, passed_exam)\n",
    "\n",
    "# Save the model coefficients and intercept here\n",
    "calculated_coefficients = model.coef_\n",
    "print(calculated_coefficients) # [[1.71391157]]\n",
    "intercept = model.intercept_\n",
    "print(intercept) # [-0.24783765]\n",
    "\n",
    "# Predict the probabilities of passing for next semester's students based on the number of hours studied\n",
    "passed_predictions = model.predict_proba(guessed_hours_scaled)\n",
    "\n",
    "# Create a new model on the training data with two features students were asked to estimate how much time they spent studying, as well as how many previous math courses they have taken\n",
    "model_2 = LogisticRegression()\n",
    "model_2.fit(exam_features_scaled_train, passed_exam_2_train)\n",
    "\n",
    "# Predict whether the students will pass here\n",
    "passed_predictions_2 = model_2.predict(exam_features_scaled_test)\n",
    "print(passed_predictions_2) # [1 1 1 1 1]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "Since our data is normalized, all features vary over the same range. Given this understanding, we can compare the feature coefficients' magnitudes and signs to determine which features have the greatest impact on class prediction, and if that impact is positive or negative.\n",
    "\n",
    " - Features with larger, positive coefficients will increase the probability of a data sample belonging to the positive class\n",
    "\n",
    " - Features with larger, negative coefficients will decrease the probability of a data sample belonging to the positive class\n",
    " \n",
    " - Features with small, positive or negative coefficients have minimal impact on the probability of a data sample belonging to the positive class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "import codecademylib3_seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from exam import exam_features_scaled, passed_exam_2\n",
    "\n",
    "# the two features in the model are the number of hours studied and the number of previous math courses taken\n",
    "# Train a sklearn logistic regression model on the normalized exam data\n",
    "model_2 = LogisticRegression()\n",
    "model_2.fit(exam_features_scaled,passed_exam_2)\n",
    "\n",
    "# Assign and update coefficients\n",
    "coefficients = model_2.coef_\n",
    "\n",
    "# with numpy's tolist() method we can convert the array into a list so as to visualize the data\n",
    "coefficients = coefficients.tolist()[0]\n",
    "\n",
    "# Plot bar graph  with matplotlib's plt.bar() method. \n",
    "plt.bar([1,2],coefficients)\n",
    "plt.xticks([1,2],['hours studied','math courses taken'])\n",
    "plt.xlabel('feature')\n",
    "plt.ylabel('coefficient')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Log reg](img/logistic-regression-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logestice Regression review\n",
    "\n",
    "The output of a Logistic Regression model is a probability that ranges from 0 to 1. The output of a Linear Regression model ranges from -∞ to +∞\n",
    "\n",
    "Logistic Regression is used to perform binary classification, predicting whether a data sample belongs to a positive (present) class, labeled 1 and the negative (absent) class, labeled 0.\n",
    "\n",
    "The Sigmoid Function bounds the product of feature values and their coefficients, known as the log-odds, to the range [0,1], providing the probability of a sample belonging to the positive class.\n",
    "\n",
    "A loss function measures how well a machine learning model makes predictions. The loss function of Logistic Regression is log-loss.\n",
    "\n",
    "A Classification Threshold is used to determine the probabilistic cutoff for where a data sample is classified as belonging to a positive or negative class. The standard cutoff for Logistic Regression is 0.5, but the threshold can be higher or lower depending on the nature of the data and the situation.\n",
    "\n",
    "Scikit-learn has a Logistic Regression implementation that allows you to fit a model to your data, find the feature coefficients, and make predictions on new data samples.\n",
    "The coefficients determined by a Logistic Regression model can be used to interpret the relative importance of each feature in predicting the class of a data sample.\n",
    "\n",
    "Checkout `sklearn`'s [Breast Cancer Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) as an exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
