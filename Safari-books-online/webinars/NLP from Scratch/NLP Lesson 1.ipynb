{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Natural Language Processing From Scratch</h1>\n",
    "<h2 align=\"center\">Bruno Gon√ßalves</h2>\n",
    "<h4 align=\"center\">bgoncalves@gmail.com</h4>\n",
    "<h4 align=\"center\">@bgoncalves</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson I - Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will see in some details how we can best represent text in our application. Let's start by importing the modules we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a well known nursery rhyme, that has the added distinction of having been the first audio ever recorded, to be the short snippet of text that we will use in our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Mary had a little lamb, little lamb,\n",
    "    little lamb. Mary had a little lamb\n",
    "    whose fleece was white as snow.\n",
    "    And everywhere that Mary went\n",
    "    Mary went, Mary went. Everywhere\n",
    "    that Mary went,\n",
    "    The lamb was sure to go\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The first step in any analysis is to tokenize the text. What this means is that we will extract all the individual words in the text. For the sake of simplicity, we will assume that our text is well formed and that our words are delimited either by white space or punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    temp = text.split() # Split the text on whitespace\n",
    "    text_words = []\n",
    "\n",
    "    for word in temp:\n",
    "        # Remove any punctuation characters present in the beginning of the word\n",
    "        while word[0] in string.punctuation:\n",
    "            word = word[1:]\n",
    "\n",
    "        # Remove any punctuation characters present in the end of the word\n",
    "        while word[-1] in string.punctuation:\n",
    "            word = word[:-1]\n",
    "\n",
    "        # Append this word into our list of words.\n",
    "        text_words.append(word.lower())\n",
    "        \n",
    "    return text_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we now have our text represented as an array of individual, lowercase, words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', 'had', 'a', 'little', 'lamb', 'little', 'lamb', 'little', 'lamb', 'mary', 'had', 'a', 'little', 'lamb', 'whose', 'fleece', 'was', 'white', 'as', 'snow', 'and', 'everywhere', 'that', 'mary', 'went', 'mary', 'went', 'mary', 'went', 'everywhere', 'that', 'mary', 'went', 'the', 'lamb', 'was', 'sure', 'to', 'go']\n"
     ]
    }
   ],
   "source": [
    "text_words = extract_words(text)\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw during the video, this is a wasteful way to represent text. We can be much more efficient by representing each word by a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {} # number of occurences of each word\n",
    "word_list = [] # position of the word in the list\n",
    "vocabulary_size = 0 # how many different words we've seen\n",
    "text_tokens = []\n",
    "\n",
    "for word in text_words:\n",
    "    # If we are seeing this word for the first time, create an id for it and added it to our word dictionary\n",
    "    if word not in word_dict:\n",
    "        word_dict[word] = vocabulary_size\n",
    "        word_list.append(word)\n",
    "        vocabulary_size += 1\n",
    "    \n",
    "    # add the token corresponding to the current word to the tokenized text.\n",
    "    text_tokens.append(word_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we were tokenizing our text, we also generated a dictionary **word_dict** that maps words to integers and a **word_list** that maps each integer to the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word list: ['mary', 'had', 'a', 'little', 'lamb', 'whose', 'fleece', 'was', 'white', 'as', 'snow', 'and', 'everywhere', 'that', 'went', 'the', 'sure', 'to', 'go'] \n",
      "\n",
      " Word dictionary:\n",
      "{'a': 2,\n",
      " 'and': 11,\n",
      " 'as': 9,\n",
      " 'everywhere': 12,\n",
      " 'fleece': 6,\n",
      " 'go': 18,\n",
      " 'had': 1,\n",
      " 'lamb': 4,\n",
      " 'little': 3,\n",
      " 'mary': 0,\n",
      " 'snow': 10,\n",
      " 'sure': 16,\n",
      " 'that': 13,\n",
      " 'the': 15,\n",
      " 'to': 17,\n",
      " 'was': 7,\n",
      " 'went': 14,\n",
      " 'white': 8,\n",
      " 'whose': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Word list:\", word_list, \"\\n\\n Word dictionary:\")\n",
    "pprint(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two datastructures already proved their usefulness when we converted our text to a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 3, 4, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 14, 0, 14, 0, 14, 12, 13, 0, 14, 15, 4, 7, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, while this representation is convenient for memory reasons it has some severe limitations. Perhaps the most important of which is the fact that computers naturally assume that numbers can be operated on mathematically (by addition, subtraction, etc) in a way that doesn't match our understanding of words.\n",
    "\n",
    "## One-hot encoding\n",
    "\n",
    "One typical way of overcoming this difficulty is to represent each word by a one-hot encoded vector where every element is zero except the one corresponding to a specific word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(word, word_dict):\n",
    "    \"\"\"\n",
    "        Generate a one-hot encoded vector corresponding to *word*\n",
    "    \"\"\"\n",
    "    \n",
    "    vector = np.zeros(len(word_dict))\n",
    "    vector[word_dict[word]] = 1\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, the word \"fleece\" would be represented by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "fleece_hot = one_hot(\"fleece\", word_dict)\n",
    "print(fleece_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector has every element set to zero, except element 6, since:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_dict[\"fleece\"])\n",
    "fleece_hot[6] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "We can now use the one-hot encoded vector for each word to produce a vector representation of our original text, by simply adding up all the one-hot encoded vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 2. 2. 4. 5. 1. 1. 2. 1. 1. 1. 1. 2. 2. 4. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "text_vector1 = np.zeros(vocabulary_size)\n",
    "\n",
    "for word in text_words:\n",
    "    hot_word = one_hot(word, word_dict)\n",
    "    text_vector1 += hot_word\n",
    "    \n",
    "print(text_vector1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we can also easily skip the encoding step at the word level by using the *word_dict* defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 2. 2. 4. 5. 1. 1. 2. 1. 1. 1. 1. 2. 2. 4. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "text_vector = np.zeros(vocabulary_size)\n",
    "\n",
    "for word in text_words:\n",
    "    text_vector[word_dict[word]] += 1\n",
    "    \n",
    "print(text_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, this approach is completely equivalent to the previous one and has the added advantage of being more efficient in terms of both speed and memory requirements.\n",
    "\n",
    "This is known as the __bag of words__ representation of the text. It should be noted that these vectors simply contains the number of times each word appears in our document, so we can easily tell that the word *mary* appears exactly 6 times in our little nursery rhyme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector[word_dict[\"mary\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more pythonic (and efficient) way of producing the same result is to use the standard __Counter__ module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mary',\n",
       " 'had',\n",
       " 'a',\n",
       " 'little',\n",
       " 'lamb',\n",
       " 'little',\n",
       " 'lamb',\n",
       " 'little',\n",
       " 'lamb',\n",
       " 'mary',\n",
       " 'had',\n",
       " 'a',\n",
       " 'little',\n",
       " 'lamb',\n",
       " 'whose',\n",
       " 'fleece',\n",
       " 'was',\n",
       " 'white',\n",
       " 'as',\n",
       " 'snow',\n",
       " 'and',\n",
       " 'everywhere',\n",
       " 'that',\n",
       " 'mary',\n",
       " 'went',\n",
       " 'mary',\n",
       " 'went',\n",
       " 'mary',\n",
       " 'went',\n",
       " 'everywhere',\n",
       " 'that',\n",
       " 'mary',\n",
       " 'went',\n",
       " 'the',\n",
       " 'lamb',\n",
       " 'was',\n",
       " 'sure',\n",
       " 'to',\n",
       " 'go']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'mary': 6,\n",
      "         'lamb': 5,\n",
      "         'little': 4,\n",
      "         'went': 4,\n",
      "         'had': 2,\n",
      "         'a': 2,\n",
      "         'was': 2,\n",
      "         'everywhere': 2,\n",
      "         'that': 2,\n",
      "         'whose': 1,\n",
      "         'fleece': 1,\n",
      "         'white': 1,\n",
      "         'as': 1,\n",
      "         'snow': 1,\n",
      "         'and': 1,\n",
      "         'the': 1,\n",
      "         'sure': 1,\n",
      "         'to': 1,\n",
      "         'go': 1})\n"
     ]
    }
   ],
   "source": [
    "# we need to tokenize the string first, extract all the words\n",
    "word_counts = Counter(text_words) # count each time a word appears\n",
    "pprint(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we can easily generate the __text_vector__ and __word_dict__ data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(word_counts.items())\n",
    "\n",
    "# Extract word dictionary and vector representation\n",
    "word_dict2 = dict([[items[i][0], i] for i in range(len(items))])\n",
    "text_vector2 = [items[i][1] for i in range(len(items))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 2., 2., 4., 5., 1., 1., 2., 1., 1., 1., 1., 2., 2., 4., 1., 1.,\n",
       "       1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector # word frequency, number of times they appear in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's take a look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vector: [6, 2, 2, 4, 5, 1, 1, 2, 1, 1, 1, 1, 2, 2, 4, 1, 1, 1, 1] \n",
      "\n",
      "Word dictionary:\n",
      "{'a': 2,\n",
      " 'and': 11,\n",
      " 'as': 9,\n",
      " 'everywhere': 12,\n",
      " 'fleece': 6,\n",
      " 'go': 18,\n",
      " 'had': 1,\n",
      " 'lamb': 4,\n",
      " 'little': 3,\n",
      " 'mary': 0,\n",
      " 'snow': 10,\n",
      " 'sure': 16,\n",
      " 'that': 13,\n",
      " 'the': 15,\n",
      " 'to': 17,\n",
      " 'was': 7,\n",
      " 'went': 14,\n",
      " 'white': 8,\n",
      " 'whose': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Text vector:\", text_vector2, \"\\n\\nWord dictionary:\")\n",
    "pprint(word_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results using this approach are slightly different than the previous ones, because the words are mapped to different integer ids but the corresponding values are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_dict.keys():\n",
    "    if text_vector[word_dict[word]] != text_vector2[word_dict2[word]]:\n",
    "        print(\"Error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are no differences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words vector representation introduced above relies simply on the frequency of occurence of each word. Following a long tradition of giving fancy names to simple ideas, this is known as __Term Frequency__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we expect the the frequency with which a given word is mentioned should correspond to the relevance of that word for the piece of text we are considering. For example, **Mary** is a pretty important word in our little nursery rhyme and indeed it is the one that occurs the most often:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(items, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's hard to draw conclusions from such a small piece of text. Let us consider a significantly larger piece of text, the first 100 MB of the english Wikipedia from: http://mattmahoney.net/dc/textdata. For the sake of convenience, text8.gz has been included in this repository in the **data/** directory. We start by loading it's contents into memory as an array of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for line in gzip.open(\"data/text8.gz\", 'rt'):\n",
    "    data.extend(line.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the most common words in this large corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(data)\n",
    "\n",
    "sorted_counts = sorted(list(counts.items()), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "for word, count in sorted_counts[:10]:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, we find that the most common words are not particularly meaningful. Indeed, this is a common occurence in Natural Language Processing. The most frequent words are typically auxiliaries required due to gramatical rules.\n",
    "\n",
    "On the other hand, there is also a large number of words that occur very infrequently as can be easily seen by glancing at the word freqency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Counter(counts.values())\n",
    "dist = list(dist.items())\n",
    "dist.sort(key=lambda x:x[0])\n",
    "dist = np.array(dist)\n",
    "\n",
    "norm = np.dot(dist.T[0], dist.T[1])\n",
    "\n",
    "plt.loglog(dist.T[0], dist.T[1]/norm)\n",
    "plt.xlabel(\"count\")\n",
    "plt.ylabel(\"P(count)\")\n",
    "plt.title(\"Word frequency distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common technique to simplify NLP tasks is to remove what are known as Stopwords, words that are very frequent but not meaningful. If we simply remove the most common 100 words, we significantly reduce the amount of data we have to consider while losing little information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set([word for word, count in sorted_counts[:100]])\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "for word in data:\n",
    "    if word not in stopwords:\n",
    "        clean_data.append(word)\n",
    "\n",
    "print(\"Original size:\", len(data))\n",
    "print(\"Clean size:\", len(clean_data))\n",
    "print(\"Reduction:\", 1-len(clean_data)/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, our dataset size was reduced almost in half!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we don't simply remove the most common words in our corpus but rather a manually curate list of stopwords. Lists for dozens of languages and applications can easily be found online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency/Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of determining of the relative importance of a word is to see how often it appears across multiple documents. Words that are relevant to a specific topic are more likely to appear in documents about that topic and much less in documents about other topics. On the other hand, less meaningful words (like **the**) will be common across documents about any subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the document frequency of a word we will need to have multiple documents. For the sake of simplicity, we will treat each sentence of our nursery rhyme as an individual document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = text.split('.')\n",
    "corpus_words = []\n",
    "\n",
    "for document in corpus_text:\n",
    "    doc_words = extract_words(document)\n",
    "    corpus_words.append(doc_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our corpus is represented as a list of word lists, where each list is just the word representation of the corresponding sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now calculate the number of documents in which each word appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_count = {}\n",
    "\n",
    "for document in corpus_words:\n",
    "    word_set = set(document)\n",
    "    \n",
    "    for word in word_set:\n",
    "        document_count[word] = document_count.get(word, 0) + 1\n",
    "\n",
    "pprint(document_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the word __Mary__ appears in all 4 of our documents, making it useless when it comes to distinguish between the different sentences. On the other hand, words like __white__ which appear in only one document are very discriminative. Using this approach we can define a new quantity, the ___Inverse Document Frequency__ that tells us how frequent a word is across the documents in a specific corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_doc_freq(corpus_words):\n",
    "    number_docs = len(corpus_words)\n",
    "    \n",
    "    document_count = {}\n",
    "\n",
    "    for document in corpus_words:\n",
    "        word_set = set(document)\n",
    "\n",
    "        for word in word_set:\n",
    "            document_count[word] = document_count.get(word, 0) + 1\n",
    "    \n",
    "    IDF = {}\n",
    "    \n",
    "    for word in document_count:\n",
    "        IDF[word] = np.log(number_docs/document_count[word])\n",
    "        \n",
    "    \n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we followed the convention of using the logarithm of the inverse document frequency. This has the numerical advantage of avoiding to have to handle small fractional numbers. \n",
    "\n",
    "We can easily see that the IDF gives a smaller weight to the most common words and a higher weight to the less frequent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = inv_doc_freq(corpus_words)\n",
    "\n",
    "pprint(IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected **Mary** has the smallest weight of all words 0, meaning that it is effectively removed from the dataset. You can consider this as a way of implicitly identify and remove stopwords. In case you do want to keep even the words that appear in every document, you can just add a 1. to the argument of the logarithm above:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log\\left[1+\\frac{N_d}{N_d\\left(w\\right)}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "When we multiply the term frequency of each word by it's inverse document frequency, we have a good way of quantifying how relevant a word is to understand the meaning of a specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus_words):\n",
    "    IDF = inv_doc_freq(corpus_words)\n",
    "    \n",
    "    TFIDF = []\n",
    "    \n",
    "    for document in corpus_words:\n",
    "        TFIDF.append(Counter(document))\n",
    "    \n",
    "    for document in TFIDF:\n",
    "        for word in document:\n",
    "            document[word] = document[word]*IDF[word]\n",
    "            \n",
    "    return TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf(corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally have a vector representation of each of our documents that takes the informational contributions of each word into account. Each of these vectors provides us with a unique representation of each document, in the context (corpus) in which it occurs, making it posssible to define the similarity of two documents, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still, however, one issue with our approach to representing text. Since we treat each word as a unique token and completely independently from all others, for large documents we will end up with many variations of the same word such as verb conjugations, the corresponding adverbs and nouns, etc. \n",
    "\n",
    "One way around this difficulty is to use stemming algorithm to reduce words to their root (or stem) version. The most famous Stemming algorithm is known as the **Porter Stemmer** and was introduced by Martin Porter in 1980 [Program 14, 130 (1980)](https://dl.acm.org/citation.cfm?id=275705)\n",
    "\n",
    "The algorithm starts by defining consonants (C) and vowels (V):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set('aeiouy')\n",
    "C = set('bcdfghjklmnpqrstvwxz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stem of a word is what is left of that word after a speficic ending has been removed. A function to do this is easy to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stem(suffix, word):\n",
    "    \"\"\"\n",
    "        Extract the stem of a word\n",
    "    \"\"\"\n",
    "    \n",
    "    if word.lower().endswith(suffix.lower()): # Case insensitive comparison\n",
    "        return word[:-len(suffix)]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also defines words (or stems) to be sequences of vowels and consonants of the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "[C](VC)^m[V]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $m$ is called the **measure** of the word and [] represent optional sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(orig_word):\n",
    "    \"\"\"\n",
    "        Calculate the \"measure\" m of a word or stem, according to the Porter Stemmer algorthim\n",
    "    \"\"\"\n",
    "    \n",
    "    word = orig_word.lower()\n",
    "\n",
    "    optV = False\n",
    "    optC = False\n",
    "    VC = False\n",
    "    m = 0\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    # We can think of this implementation as a simple finite state machine\n",
    "    # looks for sequences of vowels or consonants depending of the state\n",
    "    # in which it's in, while keeping track of how many VC sequences it\n",
    "    # has encountered.\n",
    "    # The presence of the optional V and C portions is recorded in the\n",
    "    # optV and optC booleans.\n",
    "    \n",
    "    # We're at the initial state.\n",
    "    # gobble up all the optional consonants at the beginning of the word\n",
    "    while pos < len(word) and word[pos] in C:\n",
    "        pos += 1\n",
    "        optC = True\n",
    "\n",
    "    while pos < len(word):\n",
    "        # Now we know that the next state must be a vowel\n",
    "        while pos < len(word) and word[pos] in V:\n",
    "            pos += 1\n",
    "            optV = True\n",
    "\n",
    "        # Followd by a consonant\n",
    "        while pos < len(word) and word[pos] in C:\n",
    "            pos += 1\n",
    "            optV = False\n",
    "        \n",
    "        # If a consonant was found, then we matched VC\n",
    "        # so we should increment m by one. Otherwise, \n",
    "        # optV remained true and we simply had a dangling\n",
    "        # V sequence.\n",
    "        if not optV:\n",
    "            m += 1\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple example. The word __crepusculars__ should have measure 4:\n",
    "\n",
    "[cr] (ep) (usc) (ul) (ars)\n",
    "\n",
    "and indeed it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"crepusculars\"\n",
    "print(measure(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Porter algorithm sequentially applies a series of transformation rules over a series of 5 steps (step 1 is divided in 3 substeps and step 5 in 2). The rules are only applied if a certain condition is true. \n",
    "\n",
    "In addition to possibily specifying a requirement on the measure of a word, conditions can make use of different boolean functions as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ends_with(char, stem):\n",
    "    \"\"\"\n",
    "        Checks the ending of the word\n",
    "    \"\"\"\n",
    "    return stem[-1] == char\n",
    "\n",
    "def double_consonant(stem):\n",
    "    \"\"\"\n",
    "        Checks the ending of a word for a double consonant\n",
    "    \"\"\"\n",
    "    if len(stem) < 2:\n",
    "        return False\n",
    "\n",
    "    if stem[-1] in C and stem[-2] == stem[-1]:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def contains_vowel(stem):\n",
    "    \"\"\"\n",
    "        Checks if a word contains a vowel or not\n",
    "    \"\"\"\n",
    "    return len(set(stem) & V) > 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function to apply a specific rule to a word or stem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rule(condition, suffix, replacement, word):\n",
    "    \"\"\"\n",
    "        Apply Porter Stemmer rule.\n",
    "        if \"condition\" is True replace \"suffix\" by \"replacement\" in \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    stem = get_stem(suffix, word)\n",
    "\n",
    "    if stem is not None and condition is True:\n",
    "        # Remove the suffix\n",
    "        word = stem\n",
    "\n",
    "        # Add the replacement suffix, if any\n",
    "        if replacement is not None:\n",
    "            word += replacement\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how rules can be applied. For example, this rule, from step 1b is successfully applied to __pastered__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"plastered\"\n",
    "suffix = \"ed\"\n",
    "stem = get_stem(suffix, word)\n",
    "apply_rule(contains_vowel(stem), suffix, None, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While try applying the same rule to **bled** will fail to pass the condition resulting in no change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"bled\"\n",
    "suffix = \"ed\"\n",
    "stem = get_stem(suffix, word)\n",
    "apply_rule(contains_vowel(stem), suffix, None, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more complex example, we have, in Step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"adoption\"\n",
    "suffix = \"ion\"\n",
    "stem = get_stem(suffix, word)\n",
    "apply_rule(measure(stem) > 1 and (ends_with(\"s\", stem) or ends_with(\"t\", stem)), suffix, None, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, the Porter Stemmer algorithm (for the English language) applies several dozen rules (see https://tartarus.org/martin/PorterStemmer/def.txt for a complete list). Implementing all of them is both tedious and error prone, so we abstain from providing a full implementation of the algorithm here. High quality implementations can be found in all major NLP libraries such as [NLTK](http://www.nltk.org/howto/stem.html).\n",
    "\n",
    "The dificulties of defining matching rules to arbitrary text cannot be fully resolved without the use of Regular Expressions (typically implemented as Finite State Machines like our __measure__ implementation above), a more advanced topic that is beyond the scope of this course."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
