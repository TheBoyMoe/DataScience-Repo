{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Naive Bayes Classifier\n",
    "\n",
    "\n",
    "Before we can use scikit-learn's Naive Bayes classifier, we need to first transform our data into a format that scikit-learn can process using scikit-learn's `CountVectorizer` object.\n",
    "\n",
    "First we need to create a `CountVectorizer` object and teach it the vocabulary of the training set. This is done by calling the `.fit()` method and passing it a list of strings of the words we wish to teach the algorithm.\n",
    "\n",
    "```py\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit([\"Training review one\", \"Second review\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call its `.transform()` method passing it a list of strings. It transform those strings into counts of the trained words.\n",
    "\n",
    "```py\n",
    "counts = vectorizer.transform([\"one review two review\"])\n",
    "```\n",
    "\n",
    "`counts` stores an array, `[2,1,0,0]` in this case, of the number of times that the words in the training list appeared in the transform list. Thus, the word \"review\" appeared twice, the word \"one\" appeared once, and neither \"Training\" nor \"Second\" appeared at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we know that the 2 correspondeds to review? You can print `vectorizer.vocabulary_` to see the index that each word corresponds to.\n",
    "\n",
    "```py\n",
    "print(vectorizer.vocabulary_)\n",
    "{'one': 1, 'Training': 2, 'review': 0, 'Second': 3}\n",
    "```\n",
    "\n",
    "Note: even though the word \"two\" was in our new review, there wasn't an index for it in the vocabulary. This is because \"two\" wasn't in any of the strings used in the .fit() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example - formatting the data\n",
    "\n",
    "```py\n",
    "from reviews import neg_list, pos_list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "review = \"This crib was amazing\"\n",
    "\n",
    "counter = CountVectorizer()\n",
    "counter.fit(neg_list + pos_list) # already lists\n",
    "print(counter.vocabulary_)\n",
    "\n",
    "# prints what looks like an array of all 0s, but the indices that correspond to the words \"this\", \"crib\", \"was\", and \"amazing\" should all be 1.\n",
    "review_counts = counter.transform([review])\n",
    "print(review_counts.toarray()) # print a readable format\n",
    "\n",
    "# transform the training set\n",
    "training_counts = counter.transform(neg_list + pos_list)\n",
    "print(training_counts.toarray())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn MultinomialNB Classifier\n",
    "\n",
    "Using the `MultinomialNB` classifier involves three steps:\n",
    "\n",
    "1. the classifier needs to be trained using the `.fit()` method, which takes two args, 1st is an array of data points(the formatted data already prepared) and an array of labels corresponding to each data point.\n",
    "\n",
    "2. once trained, use the `.predict()` method to predict the labels of new data points. .predict() takes a list of points that you want to classify and it returns the predicted labels of those points.\n",
    "\n",
    "3. `.predict_proba()` will return the probability of each label given a point. Takes the same list of points passed to `.predict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Using Scikit-Learn's MultinomialNB \n",
    "\n",
    "```py\n",
    "from reviews import counter, training_counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "review = \"This crib was amazing\"\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "# create your MultinomialNB classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# train the model\n",
    "# 'training_counts' are our transformed points\n",
    "# We made the training points by combining neg_list and pos_list. So the first half of the labels should be 0 (for negative) and the second half should be 1 (for positive) contains 1000 of each\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "# classify the review and return the predicted labels\n",
    "print(classifier.predict(review_counts)) # [1]\n",
    "\n",
    "# The first number printed is the probability that the review was a 0 (bad) and the second number is the probability the review was a 1 (good).\n",
    "print(classifier.predict_proba(review_counts))\n",
    "\n",
    "# TEST ================================================================================\n",
    "# review = \"This crib was amazing\"\n",
    "# --> .predict() - [1], \n",
    "# --> .predict_proba() - [[0.22699537 0.77300463]]\n",
    "\n",
    "# review = \"This crib was great amazing and wonderful\"\n",
    "# --> .predict() - [1], \n",
    "# --> .predict_proba() - [[0.04977729 0.95022271]]\n",
    "\n",
    "# review = \"This crib was absolutly awful, worst I have seen\"\n",
    "# --> .predict() - [0], \n",
    "# --> .predict_proba() - [[0.67036292 0.32963708]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We used Scikit-Learn's MultinomialNB(Naive Bayes) module to create a suppervised machine learning algorithm. Note:\n",
    "\n",
    "1. A tagged dataset is necessary to calculate the probabilities used in Bayes' Theorem.\n",
    "\n",
    "2. In this example, the features of our dataset are the words used in a product review. In order to apply Bayes' Theorem, we assume that these features are independent.\n",
    "\n",
    "3. Using Bayes' Theorem, we can find P(class|data point) for every possible class. In this example, there were two classes — positive and negative. The class with the highest probability will be the algorithm’s prediction.\n",
    "\n",
    "The following ways in which we can improve our dataset before feeding it into the Naive Bayes classifier:\n",
    "\n",
    "1. Remove all punctuation from the training set.\n",
    "\n",
    "2. Lowercase every word in the training set.\n",
    "\n",
    "3. Use a bigram or trigram model. Right now, the features of a review are individual words. For example, the features of the point \"This crib is great\" are \"This\", \"crib\", \"is\", and \"great\". If we used a bigram model, the features would be \"This crib\", \"crib is\", and \"is great\". Using a bigram model makes the assumption of independence more reasonable.\n",
    "\n",
    "These are Natural Language Processing techniques that can improve performance of a Naive Bayes Classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
