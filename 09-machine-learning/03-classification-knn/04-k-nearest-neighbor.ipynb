{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours Classifier(KNN)\n",
    "\n",
    "KNN is a classification algorithm, based on the idea that data points with similar atributes tend to fall into similar categories.\n",
    "\n",
    "Examining the graph, every data point — whether its color is red, green, or white — has an x value and a y value. As a result, it can be plotted on this two-dimensional graph.\n",
    "\n",
    "The color represents the class that the KNN algorithm is trying to classify. Data points can either have the class green or the class red. If a data point is white, this means that it doesn't have a class yet. The purpose of the algorithm is to classify these unknown points.\n",
    "\n",
    "<img src=\"img/knn-1.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, consider the expanding circle around the white point. This circle is finding the k nearest neighbors to the white point. When k = 3, the circle is fairly small. Two of the three nearest neighbors are green, and one is red. So in this case, the algorithm would classify the white point as green. However, when we increase k to 5, the circle expands, and the classification changes. Three of the nearest neighbors are red and two are green, so now the white point will be classified as red.\n",
    "\n",
    "This is the central idea behind the K-Nearest Neighbor algorithm. If you have a dataset of points where the class of each point is known, you can take a new point with an unknown class, find it's nearest neighbors, and classify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about data points\n",
    "\n",
    "A feature is a piece of information(attribute/property) associated with a data point.K-Nearest Neighbour can work with any number of features greater than 0. When thinking about features of movie data points, we might consider:\n",
    "\n",
    " - the length of the movie in minutes.\n",
    " \n",
    " - the budget of a movie in dollars.\n",
    " \n",
    " - whether the movie was directed/produced/starred a certain individual - True/False (boolen feature)\n",
    " \n",
    " - whether it was shot in black and white - True/False (boolean feature)\n",
    " \n",
    "You then need to consider how you are going to classify your data points. In this example, we're going to be classifying movies as either good or bad. In our dataset, we're going to classify a movie as good if it had an IMDb rating of 7.0 or greater. Every \"good\" movie will have a class of 1, while every bad movie will have a class of 0.\n",
    "\n",
    "Below are some movie data points where the first item in the list is the length, the second is the budget, and the third is whether the movie was directed by Stanley Kubrick.\n",
    "\n",
    "```py\n",
    "mean_girls = [97, 17000000, False]\n",
    "the_shining = [146, 19000000, True]\n",
    "gone_with_the_wind = [238, 3977000, False]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between 2D points\n",
    "\n",
    "To determine whether 2 points are close together or far apart we'll use the `Distance Formula`. When thinking about a movie, we can consider the following dimensions, it's `length` and `release date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.770329614269007\n",
      "38.897300677553446\n"
     ]
    }
   ],
   "source": [
    "# data points\n",
    "star_wars = [125, 1977]\n",
    "raiders = [115, 1981]\n",
    "mean_girls = [97, 2004]\n",
    "\n",
    "# using euclidean distance\n",
    "def distance(pt1, pt2):\n",
    "    distance = 0\n",
    "    for i in range(len(pt1)):\n",
    "        distance += (pt1[i] - pt2[i]) ** 2\n",
    "    return distance ** 0.5    \n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between Nth-D points\n",
    "\n",
    "If we were to add the movie's budget to our data points, we would have to find the the distance between two points in three dimensions. We don't have to stop there. We can add further dimensions to our data points. The generalized distance formula between points A and B is as follows:\n",
    "\n",
    "![Formula](img/formula-2.png)\n",
    "\n",
    "Using the `distance()` method above, we can find the K-Nearest Neighbors of a point in N-dimensional space! We now can use as much information about our movies as we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating K-Nearest Neighbour - Write a KNN Classifier from Scratch\n",
    " \n",
    "Implementing the KNN algorithm involves three steps:\n",
    " \n",
    "**1. Normalize the data**\n",
    " \n",
    "When you consider the movie data, we find that is easy for certain features/dimensions to overwelm/outweigh others, e.g. the difference between the release dates of movies is measuered in single or double digits, while the budgets can be hundreds of millions of dollars apart. A distance formula treats all dimensions equally, regardless of their scale. The difference in one year is treated exactly equal to the difference in one dollar of budget. This results in the budget completely outweighing the importance of all other dimensions because it is on such a huge scale. The fact that two movies were 10 years apart is essentially meaningless compared to the difference in millions in the budget dimension.\n",
    "\n",
    "The solution is to `Normalize` data so that every value is between 0 and 1. One such technique is `Min-Max` normalization. The distance formula will work with any number of dimensions/features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047619047619047616, 0.8492063492063492, 0.8650793650793651, 0.4523809523809524, 0.5634920634920635, 0.46825396825396826, 0.6666666666666666, 0.5476190476190477, 1.0, 0.36507936507936506, 0.6111111111111112, 0.8333333333333334, 0.42063492063492064, 0.0, 0.8253968253968254, 0.4523809523809524, 0.9523809523809523, 0.5873015873015873, 0.0, 0.6904761904761905]\n",
      "0.047619047619047616\n"
     ]
    }
   ],
   "source": [
    "release_dates = [1897, 1998, 2000, 1948, 1962, 1950, 1975, 1960, 2017, 1937, 1968, 1996, 1944, 1891, 1995, 1948, 2011, 1965, 1891, 1978]\n",
    "\n",
    "def min_max_normalize(lst):\n",
    "  minimum = min(lst) # 1891 == 0.0\n",
    "  maximum = max(lst) # 2017 == 1.0\n",
    "  normalized = []\n",
    "  \n",
    "  for value in lst:\n",
    "    normalized_num = (value - minimum) / (maximum - minimum)\n",
    "    normalized.append(normalized_num)\n",
    "  \n",
    "  return normalized\n",
    "\n",
    "print(min_max_normalize(release_dates))\n",
    "\n",
    "# What does the date 1897 get normalized to? \n",
    "print(min_max_normalize(release_dates)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the `k` nearest neighbour**\n",
    "\n",
    "Next we want to find the k nearest neighbors of the unclassified point. In this case we will set `k` to 5.\n",
    "\n",
    "In order to find the 5 nearest neighbors, we need to compare this new unclassified movie to every other movie in the dataset. This means we’re going to be using the distance formula again and again. We ultimately want to end up with a sorted list of distances and the movies associated with those distances, e.g. the unknown movie has a distance of 0.30 to Superman II.\n",
    "\n",
    "```py\n",
    "[\n",
    "  [0.30, 'Superman II'],\n",
    "  [0.31, 'Finding Nemo'],\n",
    "  ...\n",
    "  ...\n",
    "  [0.38, 'Blazing Saddles']\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "```py\n",
    "from movies import training_set, training_labels, validation_set, validation_labels\n",
    "\n",
    "# print(movie_dataset['Bruce Almighty'])\n",
    "# print(movie_labels['Bruce Almighty'])\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "  \n",
    "# determine the distance for every point from the 'unknown' point, sort from smallest to largest  & return k neighbors\n",
    "def classify(unknown, dataset, k):\n",
    "  distances = []\n",
    "  for i in dataset:\n",
    "    distance_to_point = distance(dataset[i], unknown)\n",
    "    distances.append([distance_to_point, i])\n",
    "  distances.sort()\n",
    "  neighbors = distances[0:k]\n",
    "  return neighbors\n",
    "  \n",
    "\n",
    "# TEST - each movie data dimension has already been normalized\n",
    "print(classify([0.4, 0.2, 0.9], training_set, 5))\n",
    "\n",
    "# k nearest neighbours - assuming k == 5\n",
    "[\n",
    "    [0.08273614694606074, 'Lady Vengeance'], \n",
    "    [0.22989623153818367, 'Steamboy'], \n",
    "    [0.23641372358159884, 'Fateless'], \n",
    "    [0.26735445689589943, 'Princess Mononoke'], \n",
    "    [0.3311022951533416, 'Godzilla 2000']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Classify the new point based on those neighbours**\n",
    "\n",
    "Next we count the number of good movies and bad movies in the list of neighbors. If more of the neighbors were good, then the algorithm will classify the unknown movie as good. Otherwise, it will classify it as bad.\n",
    "\n",
    "In order to find the class of each of the labels, we'll need to look at our `movie_labels` dataset. For example, movie_labels['Akira'] would give us 1 because Akira is classified as a good movie.\n",
    "\n",
    "If there's a tie, e.g. there are four neighbours classified as good and four as bad, we would choose the class of the closest neighbour to classify the unknown point.\n",
    "\n",
    "```py\n",
    "# Full code example\n",
    "from movies import training_set, training_labels, validation_set, validation_labels, normalize_point\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "\t\n",
    "  # classify the k closest neighbours - classify movies based on good(1) or bad(0)\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for movie in neighbors:\n",
    "    title = movie[1] # [distance, title]\n",
    "    if labels[title]:\n",
    "      num_good += 1\n",
    "    else:\n",
    "    \tnum_bad += 1\n",
    "    \n",
    "    if num_good > num_bad:\n",
    "      return 1\n",
    "  return 0\n",
    "\n",
    "# test\n",
    "print(classify([0.4, 0.2, 0.9], training_set, training_labels, 5)) # 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've built a classifier, it's time to test it with a real movie, normalize it and run it through the classifier to see what it predicts.\n",
    "\n",
    "1. check that the movie is not in our movie database\n",
    "\n",
    "```py\n",
    "# check if the film title 'Little Big Man' is in the database\n",
    "print('Little Big Man' in training_set) # False\n",
    "```\n",
    "\n",
    "2. create a datapoint for the movie: movie's budget(dollars), runtime(minutes), year released - in that order\n",
    "\n",
    "```py\n",
    "my_movie = [15000000, 139, 1970]\n",
    "```\n",
    "\n",
    "3. normalize the datapoint\n",
    "\n",
    "```py\n",
    "normalized_my_movie = normalize_point(my_movie)\n",
    "```\n",
    "\n",
    "4. classify the normalized data point\n",
    "\n",
    "```py\n",
    "print(classify(normalized_my_movie, training_set, trining_labels, 5)) \n",
    "# 1 - Good, IMBD rating 7.6\n",
    "```\n",
    "\n",
    "Movies were classified as good if it had an IMDb rating of 7.0 or greater. Every \"good\" movie will have a class of 1, while every bad movie will have a class of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our Classifier\n",
    "\n",
    "It's now time to validate the classifier and determine how effective it is by feeding it every data point in our validation set and calculating the accuracy(we know what the actual rating is for each movie in the validation set). The validation accuracy will change depending on what K value we use.\n",
    "\n",
    "```py\n",
    "print(validation_set['Bee Movie'])\n",
    "# [0.012279463360232739, 0.18430034129692832, 0.898876404494382]\n",
    "\n",
    "print(validation_labels['Bee Movie']) # 0 - actual\n",
    "\n",
    "# use the classifier to predict the movie\n",
    "guess = classify(validation_set['Bee Movie'], training_set, training_labels, 5)\n",
    "print(guess)# 0 - predicted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing K\n",
    "\n",
    "The validation accuracy changes as `k` changes.When considering the `k` value we run into the problem of `overfitting` or `underfitting`. `Overfitting` happens when we pick a small `k` value and don't consider enough neighbours, you rely too heavily on your training data and assume that data in the real world will always behave exactly like your training data. When `k` is too small, outliers will dominate the result. a single outlier could predict a datapoint to be class A even if every other point in the same area is class B.\n",
    "\n",
    "<img src=\"img/scatter-plot-4.png\" width=\"400\">\n",
    "\n",
    "Consider the black dot on the top left. A single outlier could drastically determine the label of an unknown point when you have a very small `k == 1`. All points in that general area will be classified as dark blue when it should probably be classified as green.\n",
    "\n",
    "`Underfitting` occurs when we have a very large `k` value and so consider to many points, your classifier doesn't pay enough attention to the small quirks in the training set. If you have a `k == 100` and the data set has 100 data points, then every unknown point will be classified in the same way, the distances between the points will not matter.  When `k` is too big, larger trends in the dataset aren't represented - all predictions will be exactly the same.\n",
    "\n",
    "One problem can occur when choosing an even value of `k` . In such cases, if there are equal numbers of nearest neighbours of both classes, the classifier will not know which one to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "def find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, k):\n",
    "    num_correct = 0.0\n",
    "    for title in validation_set:\n",
    "      guess = classify(validation_set[title], training_set, training_labels, k)\n",
    "      if guess == validation_labels[title]:\n",
    "        num_correct += 1\n",
    "    validation_error = num_correct / len(validation_set)  \n",
    "    return validation_error\n",
    "\n",
    "# calculate accuracy with k == 3\n",
    "print(find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, 3)) # 0.6639344262295082  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph to the right shows the validation accuracy of our movie classifier as k increases. When k is small, overfitting occurs and the accuracy is relatively low. On the other hand, when k gets too large, underfitting occurs and accuracy starts to drop.\n",
    "\n",
    "<img src=\"img/validation-accuracy.png\" width=\"600\">\n",
    "\n",
    "As k increases, you begin to avoid overfitting and accuracy goes up. Once k gets too big, you begin to underfit, and accuracy will go back down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. Data with n features can be conceptualized as points lying in n-dimensional space.\n",
    "\n",
    "2. It is essential to normalize data when calculating KNN, otherwise a feature with a vastly different scale will dominate other features.\n",
    "\n",
    "2. Data points can be compared by using the distance formula. Data points that are similar will have a smaller distance between them.\n",
    "\n",
    "3. A point with an unknown class can be classified by finding the k nearest neighbors.\n",
    "\n",
    "4. To verify the effectiveness of a classifier, data with known classes can be split into a training set and a validation set. Validation error can then be calculated.\n",
    "\n",
    "5. Classifiers have parameters that can be tuned to increase their effectiveness. In the case of K-Nearest Neighbors, k can be changed.\n",
    "\n",
    "6. A classifier can be trained improperly and suffer from overfitting or underfitting. In the case of K-Nearest Neighbors, a low k often leads to overfitting and a large k often leads to underfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
