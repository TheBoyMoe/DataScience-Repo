{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing text\n",
    "\n",
    "Some of the data comes in the form of free-form text. We'll often want to process such text to create features for our model using **NLP** - Natural Language Processing.\n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "The first step is **tokenization**, this is the process of splitting a string into a list of strings, one string for each word.\n",
    "\n",
    "For the string 'petro-vend fuel and fluids':\n",
    "\n",
    "If we tokenize on **white space** (spaces, tab or newline character), we get:\n",
    "\n",
    "Four tokens : ['petro-vend', 'fuel', 'and', 'fluids']\n",
    "\n",
    "If we were to tokenize on **white space** and **punctuation** (spaces, tab, newline character, or any mark of punctuation), we get:\n",
    "\n",
    "Five tokens: ['petro', 'vend', 'fuel', 'and', 'fluids']\n",
    "\n",
    "**Count the occurence of our tokens**\n",
    "\n",
    "Second, we need to count the number of times that a particular token occurs in a row/observation, this is called **bag of words** representation. It is the simplest technique available in order to represent text in a way(as numerical data) that our ML model can use. It is assumed that the number of times that a word is sufficient information. However, this approach does not include word order or grammer, so that information is lost.\n",
    "\n",
    "A more sophisticated approach is to create **n-grams**. In addition for creating columns with the count of each single word, **1-grams**, we may have a column, **2-grams**, for every ordered pair of two words. Thus:\n",
    "\n",
    "['petro', 'vend', 'fuel', 'and', 'fluids']\n",
    "\n",
    "1-grams : 'petro', 'vend', 'fuel', 'and', 'fluids' columns\n",
    "2-grams : 'petro vend', 'vend fuel', 'fuel and', 'and fluids' columns\n",
    "3-grams : 'petro vend fuel', 'vend fuel and', 'fuel and fluids' columns\n",
    "\n",
    "**n** can be any number.\n",
    "\n",
    "**Represent text numerically**\n",
    "\n",
    "Tools like sklearn cannot use text in their models, so we need to convert the text to numerical data. sklearn provides a **bag of words** function, the `CountVectorizer()`. It takes an array of strings and:\n",
    "\n",
    "- tokenizes all of the strings\n",
    "- builds a **vocabularly**, takes note of ALL the words/tokens that appear\n",
    "- counts the occurences of each token in each row\n",
    "\n",
    "We need to provide the function a regex that it is to use as the separator, e.g. `\\\\S+(?=\\\\s+)`, with which it can tokenize the strings.\n",
    "\n",
    "We also need to ensure that our strings do not contain any `NaN` values, and simply replace them with an empty string.\n",
    "\n",
    "We can then use the `count_vectorizer` object created with `.fit()` just like any other estimator in sklearn. `fit()` will parse all the strings for tokens and create the **vocabularly** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn's CountVectorizer method\n",
    "\n",
    "We'll look at the effects of tokenizing in different ways by comparing the **bag-of-words** representations resulting from different token patterns.\n",
    "\n",
    "We will focus on one feature only, the `Position_Extra` column, which describes any additional information not captured by the `Position_Type` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore deprecation warnings in sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import our custom train_test split function\n",
    "from multilabel import multilabel_train_test_split\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv('../data/TrainingData.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function                                         Student Transportation\n",
       "Use                                                                 O&M\n",
       "Sharing                                                 Shared Services\n",
       "Reporting                                                    Non-School\n",
       "Student_Type                                                Unspecified\n",
       "Position_Type                                                     Other\n",
       "Object_Type                                  Other Compensation/Stipend\n",
       "Pre_K                                                          Non PreK\n",
       "Operating_Status                                      PreK-12 Operating\n",
       "Object_Description        Extra Duty Pay/Overtime For Support Personnel\n",
       "Text_2                                                              NaN\n",
       "SubFund_Description                                          Operations\n",
       "Job_Title_Description                     TRANSPORTATION,BUS DR., RADIO\n",
       "Text_3                                                              NaN\n",
       "Text_4                                     transportation - Second Runs\n",
       "Sub_Object_Description    Extra Duty Pay/Overtime For Support Personnel\n",
       "Location_Description                                        Unallocated\n",
       "FTE                                                                 NaN\n",
       "Function_Description                                     Transportation\n",
       "Facility_or_Department                        Transportation Department\n",
       "Position_Extra                                               BUS DRIVER\n",
       "Total                                                           1752.45\n",
       "Program_Description                                       Undistributed\n",
       "Fund_Description                                 General Operating Fund\n",
       "Text_1                                                    EXTENDED DAYS\n",
       "Name: 8960, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[8960]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output of the item in row 8960 reveals that the `Object_Description` is overtime pay. For who? The `Position_Type` is merely \"other\", but the `Position_Extra` elaborates: \"BUS DRIVER\". The item has a number of NaN values.\n",
    "\n",
    "We'll turn the raw text in this column into a **bag-of-words** representation by creating tokens that contain only alphanumeric characters. Plus create a **bag-of-words** representation based on using whitespace only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 385 tokens in Position_Extra if we split on non-alpha numeric\n",
      "['1st', '2nd', '3rd', '4th', '56', '5th', '9th', 'a', 'ab', 'accountability', 'adaptive', 'addit', 'additional', 'adm', 'admin', 'administrative', 'adult', 'aide', 'air', 'alarm', 'alt', 'and', 'any', 'area', 'arra', 'art', 'arts', 'assessment', 'assistant', 'assistive']\n",
      "There are 415 tokens in Position_Extra if we split on white space\n",
      "['&', '(no', '(slp)', '-', '-2nd', '1st', '2nd', '3rd', '4th', '56', '5th', '9th', 'a', 'ab', 'accountability', 'adaptive', 'addit', 'additional', 'adm', 'admin', 'administrative', 'adult', 'aide', 'air', 'alarm', 'alt', 'and', 'any', 'area', 'art']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Fill NaN values in df.Position_Extra\n",
    "df.Position_Extra.fillna('', inplace=True)\n",
    "\n",
    "# Instantiate the CountVectorizer\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.Position_Extra)\n",
    "vec_basic.fit(df.Position_Extra)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:30])\n",
    "\n",
    "msg = \"There are {} tokens in Position_Extra if we split on white space\"\n",
    "print(msg.format(len(vec_basic.get_feature_names())))\n",
    "print(vec_basic.get_feature_names()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating with alpha-numeric characters as tokens generates a smaller number of more meaningful tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
