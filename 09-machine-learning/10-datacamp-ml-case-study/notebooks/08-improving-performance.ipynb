{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving performance\n",
    "\n",
    "We'll focus on improvements that can be made in these three areas:\n",
    "\n",
    "1. Text processing\n",
    "\n",
    "    - tokenize on punctuation to avoid hyphens, underscores, etc.\n",
    "    - include unigrams and bi-grams in the model. By doing so we're more likely to capture important information involving multiple tokens, e.g. 'middle school'.\n",
    "\n",
    "Sklearn facilitates this through various parameters we can pass to `CountVectorizer` in our preprocessing steps to our pipeline. We can customizer the vectorizer to only accept alphanumeric characters in tokens and include `1-gram` and `2-grams` in the vectorization.\n",
    "\n",
    "```py\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "vec = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))\n",
    "```\n",
    "\n",
    "2. Statistical methods - Interaction modeling\n",
    "\n",
    "We can use n-grams to capture sequences of words when they appear in order. This will not work when terms are not next to each other, e.g. '3rd grade - budget for English teacher' and 'English teacher for 3rd grade'. `Interaction Terms` allows us to mathematically describe when terms appear togeher. Sklearn implements `Interaction Terms` through it's `Polynomial Features` function.\n",
    "\n",
    "`degree` - number of columns to include, performance hit if too many features included\n",
    "`interaction_only=True` - tells sklearn not to multiply a column by itself.\n",
    "\n",
    "Insert the step after the preprocessing steps, but before the classifier steps.\n",
    "\n",
    "```py\n",
    "from sklearn.preprocessing import Polynomialfeatures\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "```\n",
    "\n",
    "3. Computational effifcency using a hashing function\n",
    "\n",
    "As the number of features in our model grows, the size of the resulting matrix created also grows as does the length of time and amount of memeory required.\n",
    "\n",
    "Hashing is a way of increasing memory efficiency, without sacrificing model accuracy. The hashing function takes an input, the token, and outputs a hash value, which could be a string or integer. We explicitly state how many outputs the hashing function can have, e.g. 250. The hashing function will automatically map each token to one of the 250 values/columns. Some columns will have more than one token mapped to it. This does not affect model accuracy.\n",
    "\n",
    "By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets.\n",
    "\n",
    "Hashing is used in **Dimensionality Reduction**, making the array of features as small as possible. Which is particularly useful when the dataset is particularly large.\n",
    "\n",
    "Sklearn implements hashing through the `HashingVectorizer`, which we can use instead of the `CountVectorizer`.\n",
    "\n",
    "```py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vec = Hashingvectorizer(\n",
    "    norm=None,\n",
    "    non_negative=True,\n",
    "    token_pattern=TOKENS_ALPHANUMERIC,\n",
    "    ngram_range(1, 2)\n",
    ")\n",
    "```\n",
    "\n",
    "A `HashingVectorizer` acts just like `CountVectorizer` in that it can accept `token_pattern` and `ngram_range` parameters. The important difference is that it creates hash values from the text, so that we get all the computational advantages of hashing!\n",
    "\n",
    "```py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Get text data: text_data\n",
    "text_data = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Instantiate the HashingVectorizer: hashing_vec\n",
    "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit and transform the Hashing Vectorizer\n",
    "hashed_text = hashing_vec.fit_transform(text_data)\n",
    "\n",
    "# Create DataFrame and print the head\n",
    "hashed_df = pd.DataFrame(hashed_text.data)\n",
    "print(hashed_df.head())\n",
    "```\n",
    "\n",
    "```py\n",
    "          0\n",
    "0 -0.160128\n",
    "1  0.160128\n",
    "2 -0.480384\n",
    "3 -0.320256\n",
    "4  0.160128\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further steps to improve the model:\n",
    "\n",
    "- further NLP techniques, such as stemming and stop-word removal\n",
    "- try a different model, e.g. `RandomForest`, `KNN`, `Naive Bayes`, etc.\n",
    "- further numeric preprocessing, e.g. different imputation strategy other than the default of using `NaN`.\n",
    "- further optimsation, such as using a `GridSearch` over pipeline objects.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
