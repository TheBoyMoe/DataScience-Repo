{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Building a Linear Regression Algorithm from Scratch\n",
    "\n",
    "The purpose of machine learning is to create a model, that given an input is able to predict the output. The simplest model that we can fit to data is a straight line. Trying to find a line that fits a set of data is called **Linear Regression**, e.g. if we were to plot house prices vs square footage, can we predict the price given it's size?\n",
    "\n",
    "If we plotted weights against heights of a set of professional baseball players:\n",
    "![Scatter Plot 1](img/scatter-plot-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to draw a line to best fit the data:\n",
    "![Scatter plot 2](img/scatter-plot-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we wanted to estimate the weight of a player with a height of 73 inches, we could estimate that it is around 143 pounds. A line allows us the ability to explain and predict variables that have a linear relationship with each other. We're looking for the best fit.\n",
    "\n",
    "A line is determined by its slope(a measure of how steep the line is) and its intercept(a measure of where the line hits the y-axis). For each point y on a line we can say:\n",
    "\n",
    "```py\n",
    "y = mx + b\n",
    "```\n",
    "\n",
    " - `y` is the point on the y-axis\n",
    " - `m` is the slope\n",
    " - `b` is the intercept\n",
    " - `x` is the point on the x-axis\n",
    "\n",
    "You calculate the predicted values of `y`, given the values of `x`, `m` and `b`. Then plot the `predicted y-values` vs x and see how close the line is to fitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1667460860>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEzBJREFUeJzt3XGQ3OV93/H3p0I2h1rncHW46AQVychKHXAieiXETFMKcYWpx9IwyQxuWqsOraYZaieeVjYa/0E70ym08jS1x1M6GpsAUwaHUkUwtR2Z4LR00oDnsByEwQqMncCdsHUOFe3AGQvl2z9uzz7ESbe3u7er/d37NXOzu9/93e53QXx49Pye/T2pKiRJzfWXBt2AJGllGfSS1HAGvSQ1nEEvSQ1n0EtSwxn0ktRwBr0kNZxBL0kNZ9BLUsOdM+gGANavX1+bNm0adBuSNFSeeOKJ71fV2FLHnRVBv2nTJiYnJwfdhiQNlSR/1s5xTt1IUsMZ9JLUcAa9JDWcQS9JDWfQS1LDnRWrbiRpVdm7GV459ub6ugtg97M9fztH9JLUb4uF/JnqXTLoJanhDHpJajiDXpIazqCXpIZbMuiT3JnkWJKnFtR+LsljSb6RZDLJFa16knwmyXNJnkxy+Uo2L0lDad0Fy6t3qZ3llXcBnwXuWVD798C/rqovJ7m+9fhq4H3A5tbPzwN3tG4lSfNWYAnlmSw5oq+qR4GXTi0Db2vd/wngaOv+duCemvMYMJrkwl41K0lavk6/MPWbwMEkn2LufxbvadXHgRcWHDfVqr146gsk2QXsArj44os7bEOStJROT8b+OvCxqroI+Bjw+VY9ixxbi71AVe2rqomqmhgbW/K6+ZKkDnUa9DuB/a37/xW4onV/CrhowXEb+fG0jiRpADoN+qPA32ndvwaYP7PwEPCh1uqbK4GXq+pN0zaSpP5Zco4+yX3MrahZn2QKuBX4p8Cnk5wD/IDWXDvwJeB64DngVeDDK9CzJGkZlgz6qvrgaZ76m4scW8DN3TYlSeodvxkrSQ1n0EtSwxn0ktRw7jAlSX3e8anfHNFLUp93fOo3g16SGs6gl6SGM+glqeEMeklqOINekvq841O/ubxSkhqwhPJMHNFLUsMZ9JLUcAa9JDWcQS9JDWfQS1LDGfSS1HAGvSQ1nEEvSQ1n0EtSwxn0ktRwS14CIcmdwPuBY1V16YL6R4B/DrwOfLGqPt6q7wFuAk4CH62qgyvRuKQGa/iOT/3WzrVu7gI+C9wzX0jyd4HtwLur6rUkF7Tq7wJuBH4G2AD8fpJ3VtXJXjcuqcEavuNTvy05dVNVjwIvnVL+deD2qnqtdcz8P/3twBeq6rWq+g7wHHBFD/uVJC1Tp3P07wT+dpLHk/zPJH+rVR8HXlhw3FSr9iZJdiWZTDI5MzPTYRuSpKV0GvTnAOcDVwK7gfuTBMgix9ZiL1BV+6pqoqomxsbGOmxDkrSUToN+Cthfc74G/AWwvlW/aMFxG4Gj3bUoSepGp0F/ALgGIMk7gbcA3wceAm5M8tYklwCbga/1olFJq8gAdnw6cGiaq27/Kpfc8kWuuv2rHDg0vWLv1W/tLK+8D7gaWJ9kCrgVuBO4M8lTwA+BnVVVwDeT3A88zdyyy5tdcSNp2fq8hPLAoWn27D/M7Im5uJo+Psue/YcB2LF10dOMQyVz+TxYExMTNTk5Oeg2JK1SV93+VaaPz76pPj46wh/ecs0AOmpPkieqamKp4/xmrKRV7+giIX+m+rAx6CWtehtGR5ZVHzYGvaRVb/e2LYysXfOG2sjaNezetmVAHfVWO5dAkKRGmz/huvfgEY4en2XD6Ai7t21pxIlYMOglCZgL+6YE+6mcupGkhjPoJanhDHpJajiDXpIazpOxktrjrk9DyxG9pPa469PQMuglqeGcupF0VjpwaLqxX2DqN4Ne0lmn6ZcN7jenbiSddfYePPKjkJ83e+Ikew8eGVBHw82gl9SWP2d0WfVuNP2ywf3m1I2ktvyv7f/7DdMpMHeFx9tuuIwdPX6vDaMji24E0pTLBvebI3pJbdmxdZzbbriM8dERwtzuS7fdcNmKzJk3/bLB/eaIXlLb+nWFx6ZfNrjfDHpJZ6UmXza435acuklyZ5JjSZ5a5Ll/maSSrG89TpLPJHkuyZNJLl+JpiVJ7Wtnjv4u4LpTi0kuAt4LPL+g/D5gc+tnF3BH9y1KkrqxZNBX1aPAS4s89VvAx4FaUNsO3FNzHgNGk1zYk04lSR3paNVNkg8A01X1x6c8NQ68sODxVKsmSRqQZZ+MTXIe8Eng7y329CK1WqRGkl3MTe9w8cUXL7cNSVKbOhnR/xRwCfDHSf4U2Ah8PclfY24Ef9GCYzcCRxd7karaV1UTVTUxNjbWQRuSpHYsO+ir6nBVXVBVm6pqE3PhfnlVfRd4CPhQa/XNlcDLVfVib1uWJC3HklM3Se4DrgbWJ5kCbq2qz5/m8C8B1wPPAa8CH+5Rn5JO5Y5PatOSQV9VH1zi+U0L7hdwc/dtSVqSOz6pTV7rRpIazqCXpIYz6CWp4Qx6SWo4g14aVusuWF5dq5aXKZaGlUso1SZH9JLUcAa9JDWcQS9JDeccvdRDBw5Nu8+pzjoGvdQjBw5Ns2f/YWZPnARg+vgse/YfBjDsNVBO3Ug9svfgkR+F/LzZEyfZe/DIgDqS5hj0Uo8cPT67rLrUL07dSD2yYXSE6UVCfcPoyIq9p+cE1A5H9FKP7N62hZG1a95QG1m7ht3btqzI+82fE5g+Pkvx43MCBw5Nr8j7aXgZ9FKP7Ng6zm03XMb46AgBxkdHuO2Gy1ZshO05AbXLqRupV/ZuZscrx9gBcC7wA+BB4PdXZscnzwmoXY7opV7p845Pp5v7X8lzAhpOBr00pPp9TkDDy6kbaUjNz/276kZLMeilIbZj67jBriUtOXWT5M4kx5I8taC2N8m3kjyZ5HeTjC54bk+S55IcSbJtpRqXJLWnnTn6u4DrTqk9DFxaVe8G/gTYA5DkXcCNwM+0fuc/JVmDtBq445POUktO3VTVo0k2nVL7yoKHjwG/3Lq/HfhCVb0GfCfJc8AVwB/1pFvpbOaOTzpL9WLVza8BX27dHwdeWPDcVKsmSRqQroI+ySeB14F750uLHFan+d1dSSaTTM7MzHTThiTpDDoO+iQ7gfcDv1pV82E+BVy04LCNwNHFfr+q9lXVRFVNjI2NddqGJGkJHQV9kuuATwAfqKpXFzz1EHBjkrcmuQTYDHyt+zYlSZ1a8mRskvuAq4H1SaaAW5lbZfNW4OEkAI9V1T+rqm8muR94mrkpnZur6uTiryxJ6of8eNZlcCYmJmpycnLQbUjSUEnyRFVNLHWc34xV37lZhtRfBr36yg20pf7z6pXqKzfLkPrPoFdfuVmG1H9O3aiv+r6B9t7Ni2/8sW5ldn2SzkaO6NVXfd8so8+7PklnI0f06is3y5D6z6BX37lZhtRfTt1IUsMZ9JLUcAa9Gm2mfmJZdamJnKNXo+0YuWvR5ZzjoyP84QD6kQbBEb0are/LOaWzkCN6NZrLOSWDXquAyzm12jl1I0kNZ9BLUsMZ9JLUcAa9JDWcQS9JDWfQS1LDLRn0Se5McizJUwtqb0/ycJJnW7fnt+pJ8pkkzyV5MsnlK9m8JGlp7ayjvwv4LHDPgtotwCNVdXuSW1qPPwG8D9jc+vl54I7WrTTHHZ+kvltyRF9VjwIvnVLeDtzdun83sGNB/Z6a8xgwmuTCXjWrBnDHJ6nvOp2jf0dVvQjQur2gVR8HXlhw3FSrJkkakF6fjM0itVr0wGRXkskkkzMzMz1uQ5I0r9Og/978lEzrdv7v3VPARQuO2wgcXewFqmpfVU1U1cTY2FiHbUiSltJp0D8E7Gzd3wk8uKD+odbqmyuBl+eneCRJg7Hkqpsk9wFXA+uTTAG3ArcD9ye5CXge+JXW4V8CrgeeA14FPrwCPWuYrbvg9KtuJK2IJYO+qj54mqeuXeTYAm7utik1mEsopb7zevTiwKFpN+aQGsygX+UOHJpmz/7DzJ44CcD08Vn27D8MYNhLDeG1bla5vQeP/Cjk582eOMneg0cG1JGkXjPoV7mjx2eXVZc0fAz6VW7D6Miy6pKGj0G/yu3etoWRtWveUBtZu4bd27YMqCNJvebJ2FVu/oSrq26k5jLoxY6t4wa71GBO3UhSwxn0ktRwTt2sdu74JDWeI/rVzh2fpMYz6CWp4Qx6SWo4g16SGs6gl6SGM+hXu9Pt7OSOT1JjuLxytXMJpdR4juglqeEMeklqOINekhquq6BP8rEk30zyVJL7kpyb5JIkjyd5NsnvJHlLr5qVJC1fx0GfZBz4KDBRVZcCa4AbgX8H/FZVbQb+D3BTLxqVJHWm21U35wAjSU4A5wEvAtcA/6D1/N3AvwLu6PJ9Vp0Dh6bdDERST3Q8oq+qaeBTwPPMBfzLwBPA8ap6vXXYFGA6LdOBQ9Ps2X+Y6eOzFDB9fJY9+w9z4ND0oFuTNIS6mbo5H9gOXAJsANYB71vk0DrN7+9KMplkcmZmptM2GmnvwSPMnjj5htrsiZPsPXhkQB1JGmbdnIz9JeA7VTVTVSeA/cB7gNEk81NCG4Gji/1yVe2rqomqmhgbG+uijeY5enx2WXVJOpNugv554Mok5yUJcC3wNPAHwC+3jtkJPNhdi6vPhtGRZdUl6Uw6PhlbVY8neQD4OvA6cAjYB3wR+EKSf9Oqfb4Xja4mj9Q/4dxz//xN9R/UXwW+3f+GJA21rlbdVNWtwK2nlL8NXNHN665257725pA/U12SzsRvxkpSw3n1yja5rl3SsDLo2zC/rn1+yeP8unbAsJd01nPqpg2ua5c0zAz6NvR9Xbu7PknqIadu2rBhdITpRUJ9xda1u+uTpB5yRN+G3du2MLJ2zRtqI2vXsHvblgF1JEntc0TfhvkTrq66kTSMDPo27dg6brBLGkpO3UhSwxn0ktRwBr0kNZxBL0kNZ9BLUsMZ9JLUcAa9JDWc6+jbsXczvHLszfV1F3i5AklnPUf07Vgs5M9Ul6SziEEvSQ1n0EtSwxn0ktRwXQV9ktEkDyT5VpJnkvxCkrcneTjJs63b83vVrCRp+bod0X8a+L2q+mngZ4FngFuAR6pqM/BI6/Fwc8cnSUOs4+WVSd4G/CLwjwGq6ofAD5NsB65uHXY38D+AT3TT5MC5hFLSEOtmRP+TwAzw20kOJflcknXAO6rqRYDWrcNeSRqgboL+HOBy4I6q2gq8wjKmaZLsSjKZZHJmZqaLNiRJZ9JN0E8BU1X1eOvxA8wF//eSXAjQul30W0VVta+qJqpqYmxsrIs2JEln0nHQV9V3gReSzO+QfS3wNPAQsLNV2wk82FWHkqSudHutm48A9yZ5C/Bt4MPM/c/j/iQ3Ac8Dv9Lle0iSutBV0FfVN4CJRZ66tpvXlST1jt+MlaSGM+glqeEMeklqOINekhpuOHeYcscnSWrbcI7o3fFJkto2nEEvSWqbQS9JDWfQS1LDGfSS1HDDGfTu+CRJbRvO5ZUuoZSktg3niF6S1DaDXpIazqCXpIYz6CWp4Qx6SWq4VNWgeyDJDPBng+6jTeuB7w+6iRXS5M8Gzf58frbh1c3n++tVNbbUQWdF0A+TJJNVtdj2iUOvyZ8Nmv35/GzDqx+fz6kbSWo4g16SGs6gX759g25gBTX5s0GzP5+fbXit+Odzjl6SGs4RvSQ1nEHfpiQXJfmDJM8k+WaS3xh0T72WZE2SQ0n++6B76aUko0keSPKt1r+/Xxh0T72U5GOtP5NPJbkvybmD7qlTSe5McizJUwtqb0/ycJJnW7fnD7LHbpzm8+1t/dl8MsnvJhnt9fsa9O17HfgXVfU3gCuBm5O8a8A99dpvAM8MuokV8Gng96rqp4GfpUGfMck48FFgoqouBdYANw62q67cBVx3Su0W4JGq2gw80no8rO7izZ/vYeDSqno38CfAnl6/qUHfpqp6saq+3rr//5gLi/HBdtU7STYCfx/43KB76aUkbwN+Efg8QFX9sKqOD7arnjsHGElyDnAecHTA/XSsqh4FXjqlvB24u3X/bmBHX5vqocU+X1V9papebz18DNjY6/c16DuQZBOwFXh8sJ301H8EPg78xaAb6bGfBGaA325NS30uybpBN9UrVTUNfAp4HngReLmqvjLYrnruHVX1IswNuIAm7zD0a8CXe/2iBv0yJfnLwH8DfrOq/u+g++mFJO8HjlXVE4PuZQWcA1wO3FFVW4FXGO6/+r9Ba756O3AJsAFYl+QfDrYrdSLJJ5mbIr63169t0C9DkrXMhfy9VbV/0P300FXAB5L8KfAF4Jok/2WwLfXMFDBVVfN/+3qAueBvil8CvlNVM1V1AtgPvGfAPfXa95JcCNC6PTbgfnouyU7g/cCv1gqseTfo25QkzM3zPlNV/2HQ/fRSVe2pqo1VtYm5E3lfrapGjAqr6rvAC0m2tErXAk8PsKVeex64Msl5rT+j19Kgk80tDwE7W/d3Ag8OsJeeS3Id8AngA1X16kq8h0HfvquAf8TcaPcbrZ/rB92U2vIR4N4kTwI/B/zbAffTM62/qTwAfB04zNx/00P7TdIk9wF/BGxJMpXkJuB24L1JngXe23o8lE7z+T4L/BXg4Vau/Oeev6/fjJWkZnNEL0kNZ9BLUsMZ9JLUcAa9JDWcQS9JDWfQS1LDGfSS1HAGvSQ13P8HaIa/lukhkuYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]\n",
    "\n",
    "#slope:\n",
    "m = 12\n",
    "#intercept:\n",
    "b = 40\n",
    "plt.plot(months, revenue, \"o\")\n",
    "\n",
    "# caluculate y values given x-values, slope and intercept\n",
    "y = [m * months + b for months in months]\n",
    "plt.plot(months, y, 's')\n",
    "\n",
    "# change the m and  b values so the line fits the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "When we think about how we can assign a slope and intercept to fit a set of points, we have to define what the best fit is. For each data point, we calculate loss(or error), a number that measures how bad the model's (in this case, the line's) prediction was.\n",
    "It's the squared distance (instead of just the distance) so that points above and below the line both contribute to total loss:\n",
    "\n",
    "![Loss](img/loss.png)\n",
    "\n",
    "For point A, the squared distance is 9 (3²). For point B, the squared distance is 1 (1²). The totla loss for this model is `10`.  If we found a line that had less loss than 10, that line would be a better model for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "17\n",
      "13.5\n"
     ]
    }
   ],
   "source": [
    "# calculate line of best fit\n",
    "x = [1, 2, 3]\n",
    "y = [5, 1, 3]\n",
    "\n",
    "#y = x\n",
    "m1 = 1\n",
    "b1 = 0\n",
    "# predict the values of y, based on the values m1, b and x\n",
    "y_predicted1 = [m1 * x_value + b1 for x_value in x]\n",
    "print(y_predicted1)\n",
    "\n",
    "#y = 0.5x + 1\n",
    "m2 = 0.5\n",
    "b2 = 1\n",
    "y_predicted2 = [m2 * x_value + b2 for x_value in x]\n",
    "\n",
    "total_loss1 = 0\n",
    "\n",
    "# find the sum of the squared distance between the actual y-values of the points and the y_predicted1 values\n",
    "for i in range(len(y)):\n",
    "  total_loss1 += (y[i] - y_predicted1[i]) ** 2\n",
    "\n",
    "total_loss2 = 0\n",
    "for i in range(len(y)):\n",
    "  total_loss2 += (y[i] - y_predicted2[i]) ** 2\n",
    "\n",
    "print(total_loss1)\n",
    "print(total_loss2) # best fit, smallest loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a linear regression model is to find the slope and intercept pair (`b` and `m` values) that minimizes loss on average across all of the data using `Gradient Descent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Intercept\n",
    "\n",
    "As we try to minimize loss, we take each parameter we are changing(`m` - slope, and `b` - intercept), and move it as long as we are decreasing loss. The process by which we do this is called gradient descent. We move in the direction(which can involve either decreasing/increasing the parameter) that decreases our loss the most. Gradient refers to the slope of the curve at any point.\n",
    "\n",
    "Let's say we are trying to find the intercept for a line. We currently have a guess of 10 for the intercept. At the point of 10 on the curve, the slope is downward. Therefore, if we increase the intercept, we should be lowering the loss. So we follow the gradient downwards.\n",
    "\n",
    "![Linear regression](img/linear-regression.gif)\n",
    "\n",
    "To derive the gradient at various points we:\n",
    "\n",
    "1. find the sum of `y_value - (m * x_value + b)` for all `y_values` and `x-values`.\n",
    "\n",
    "2. multiply the sum by a factor of `-2/N`, where `N` is the number of points.\n",
    "\n",
    "We can derive the gradient of `b` at each point using this formula:\n",
    "\n",
    "```py\n",
    "# x - x values, y - y values, b - slope, b - intercept\n",
    "# following the formula: gradient = y - (m * x + b)\n",
    "# N number of points\n",
    "def get_gradient_at_b(x, y, m, b):\n",
    "  diff = 0\n",
    "  N = len(x)\n",
    "  for i in range(0, len(x)):\n",
    "    y_val = y[i]\n",
    "    x_val = x[i]\n",
    "    diff += y_val - (m * x_val + b)\n",
    "    \n",
    "  b_gradient = -2/N * diff  \n",
    "  return b_gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Slope\n",
    "\n",
    "To find the gradient of `m` at each point:\n",
    "\n",
    "1. find the sum of `x_value * (y_value - (m * x_value + b))` for all `x_values` and `y_values`.\n",
    "\n",
    "2. multiply the sum by a factor of `-2/N` where `N` is the number of points\n",
    "\n",
    "```py\n",
    "def get_gradient_at_m(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "      x_val = x[i]\n",
    "      y_val = y[i]\n",
    "      diff += x_val * (y_val - ((m * x_val) + b))\n",
    "\n",
    "m_gradient = -(2/N) * diff  \n",
    "return m_gradient\n",
    "```\n",
    "\n",
    "Once we have a way to calculate both the m gradient and the b gradient, we'll be able to follow both of those gradients downwards to the point of lowest loss for both the m value and the b value. Then, we'll have the best m and the best b to fit our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the gradient, we want to take small steps otherwise we will 'overshoot' the minimum error. We can scale the size of the step by multiplying the gradient by a `learning rate`. Thus to find a new `b` value we would:\n",
    "\n",
    "```py\n",
    "new_b = current_b - (learning_rate * b_gradient)\n",
    "```\n",
    "\n",
    "Where `current_b` is our guess for what the `b` value is, `b_gradient` is the gradient of the loss curve at our current guess, and `learning_rate` is proportional to the size of the step we want to take.\n",
    "\n",
    "We can put it all together and create a formula to calculate the `step gradient`:\n",
    "\n",
    "\n",
    "```py\n",
    "def step_gradient(x, y, b_current, m_current):\n",
    "  b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n",
    "  m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n",
    "  b = b_current - (0.01 * b_gradient)\n",
    "  m = m_current - (0.01 * m_gradient)\n",
    "  return [b, m]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example bringing it all togther\n",
    "def get_gradient_at_b(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "    x_val = x[i]\n",
    "    y_val = y[i]\n",
    "    diff += (y_val - ((m * x_val) + b))\n",
    "  b_gradient = -(2/N) * diff  \n",
    "  return b_gradient\n",
    "\n",
    "def get_gradient_at_m(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "      x_val = x[i]\n",
    "      y_val = y[i]\n",
    "      diff += x_val * (y_val - ((m * x_val) + b))\n",
    "  m_gradient = -(2/N) * diff  \n",
    "  return m_gradient\n",
    "\n",
    "def step_gradient(x, y, b_current, m_current):\n",
    "  b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n",
    "  m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n",
    "  b = b_current - (0.01 * b_gradient)\n",
    "  m = m_current - (0.01 * m_gradient)\n",
    "  return (b, m)\n",
    "\n",
    "# sales data\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]\n",
    "\n",
    "# current intercept guess:\n",
    "b = 0\n",
    "# current slope guess:\n",
    "m = 0\n",
    "\n",
    "# update b and m\n",
    "b, m = step_gradient(months, revenue, b, m)\n",
    "\n",
    "print(b, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "\n",
    "How do we know when we should stop changing the parameters m and b? Or when our 'program' has learned enough?\n",
    "\n",
    "When the loss stops changing (or changes very slowly) when parameters are changed - the point of **Convergence**. Hopefully, the algorithm will converge at the best values for the parameters m and b. The linear regression algorithm stops when the parameters stop changing (or change very slowly).\n",
    "\n",
    "From the plot below we can see that it tock apprx. 800 iterations to reach convergence, a `b` value of 47.\n",
    "\n",
    "![Convergence](img/convergence.png)\n",
    "\n",
    "If the algorithm is taking too long to converge, should you move the learning rate up. Gradient descent is already being performed in steps that are too small, and lowering the learning rate would only make the steps smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "We want our program to be able to iteratively learn what the best `m` and `b` values are. So for each `m` and `b` pair that we guess, we want to move them in the direction of the gradients we've calculated. But how far do we move in that direction?\n",
    "\n",
    "We have to choose a learning rate, which will determine how far down the loss curve we go.\n",
    "\n",
    "To small and it will take too long to converge — you might run out of time or cycles before getting an answer. A large learning rate might skip over the best value, never converging.\n",
    "\n",
    "![Converge](img/linear-regression-2.gif)\n",
    "\n",
    "Finding the absolute best learning rate is not necessary for training a model. You just have to find a learning rate large enough that gradient descent converges with the efficiency you need, and not so large that convergence never happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puting it all together!\n",
    "\n",
    "At each step, we know how to calculate the gradient and move in that direction with a step size proportional to our learning rate. Now, we want to make these steps until we reach convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb89092d7b8>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_gradient_at_b(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "    x_val = x[i]\n",
    "    y_val = y[i]\n",
    "    diff += (y_val - ((m * x_val) + b))\n",
    "  b_gradient = -(2/N) * diff  \n",
    "  return b_gradient\n",
    "\n",
    "def get_gradient_at_m(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "      x_val = x[i]\n",
    "      y_val = y[i]\n",
    "      diff += x_val * (y_val - ((m * x_val) + b))\n",
    "  m_gradient = -(2/N) * diff  \n",
    "  return m_gradient\n",
    "\n",
    "# updated step_gradient function, added 'learning_rate'\n",
    "def step_gradient(b_current, m_current, x, y, learning_rate):\n",
    "    b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n",
    "    m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n",
    "    b = b_current - (learning_rate * b_gradient)\n",
    "    m = m_current - (learning_rate * m_gradient)\n",
    "    return [b, m]\n",
    "  \n",
    "# gradient_descent function\n",
    "def gradient_descent(x, y, learning_rate, num_iterations):\n",
    "  b= 0\n",
    "  m = 0\n",
    "  for i in range(num_iterations):\n",
    "    b, m = step_gradient(b, m, x, y, learning_rate)\n",
    "    \n",
    "  return (b, m)\n",
    "\n",
    "\n",
    "# Example ===================================================\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]\n",
    "\n",
    "# calculate the 'best' intercept and slope (b and m) for the data sets\n",
    "b, m = gradient_descent(months, revenue, 0.01, 1000)\n",
    "\n",
    "# plot the line\n",
    "y = [m*x + b for x in months]\n",
    "plt.plot(months, revenue, \"o\")\n",
    "plt.plot(months, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "\n",
    "```py\n",
    "from gradient_descent_funcs import gradient_descent\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"heights.csv\")\n",
    "\n",
    "X = df[\"height\"]\n",
    "y = df[\"weight\"]\n",
    "\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "# calculate every value of y for x, b and m\n",
    "b, m = gradient_descent(X, y, 0.0001, 1000)\n",
    "y_predictions = [i * m + b for i in X]\n",
    "\n",
    "# plot y_predictions vs x values\n",
    "plt.plot(X, y_predictions)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Scatter plot](img/scatter-plot-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a linear regression algorithm is to find the intercept and slope that minimize average loss - produce the line of best fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
