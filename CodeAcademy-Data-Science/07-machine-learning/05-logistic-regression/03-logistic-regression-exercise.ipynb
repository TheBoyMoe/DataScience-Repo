{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regrssion Exercise\n",
    "\n",
    "Create a Logistic Regression model that predicts which passengers survived the sinking of the Titanic, based on features like age and class.\n",
    "\n",
    "Data source: [Kaggle](https://www.kaggle.com/c/titanic)\n",
    "\n",
    "```py\n",
    "\n",
    "import codecademylib3_seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the passenger data\n",
    "passengers = pd.read_csv('passengers.csv')\n",
    "# print(passengers.head())\n",
    "\n",
    "# Update sex column to numerical, female == 1, male == 0\n",
    "passengers['Sex'] = passengers.Sex.map({'male': 0, 'female': 1})\n",
    "# print(passengers.head())\n",
    "\n",
    "# Fill the nan values in the age column with the mean age inplace, modify 'Age' directly, do not return new column\n",
    "# print(passengers['Age'].values) # output age values as a list\n",
    "passengers['Age'].fillna(inplace=True, value=round(passengers.Age.mean()))\n",
    "# print(passengers.Age.values)\n",
    "      \n",
    "# Create a first class column, 1 if the passenger is in Pclass == 1, 0 otherwise\n",
    "passengers['FirstClass'] = passengers.Pclass.apply(lambda p: 1 if p == 1 else 0)\n",
    "\n",
    "# Create a second class column, stores 1 for all passengers in second class and 0 for all other passengers.\n",
    "passengers['SecondClass'] = passengers.Pclass.apply(lambda p: 1 if p == 2 else 0)\n",
    "print(passengers)\n",
    "\n",
    "# Select the desired features\n",
    "# Select columns Sex, Age, FirstClass, and SecondClass and store them in a variable named features.\n",
    "features = passengers[['Sex', 'Age', 'FirstClass', 'SecondClass']] # raw data\n",
    "\n",
    "# Select column Survived and store it a variable named survival.\n",
    "survival = passengers['Survived'] # answers/labels\n",
    "\n",
    "# Perform train, test, split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, survival)\n",
    "\n",
    "# Scale the feature data so it has mean = 0 and standard deviation = 1. Since sklearn's Logistic Regression implementation uses Regularization, we need to normalize our data\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Create and train the model to recognise which passengers survived and which didn't based on our four features\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features_scaled, train_labels)\n",
    "\n",
    "# Score the model on the train data, the score returned is the percentage of correct classifications, or the accuracy.\n",
    "print(model.score(train_features_scaled, train_labels))\n",
    "# 0.7979041916167665 => 80% accuracy rate\n",
    "\n",
    "# Score the model on the test data\n",
    "print(model.score(test_features_scaled, test_labels))\n",
    "# 0.8026905829596412\n",
    " \n",
    "# Analyze the coefficients, how significant were each of the 4 features in determinig whether someone would survive\n",
    "print(model.coef_)\n",
    "# [[ 1.24283291 -0.48829518  1.03291324  0.54043375]]\n",
    "# Gender and FirstClass were the most significant factors\n",
    "\n",
    "# Sample passenger features, array order, 'Sex' (0 == male, 1 == female), 'Age', 'FirstClass', 'SecondClass' - all values must be floats\n",
    "Jack = np.array([0.0,20.0,0.0,0.0]) # 3rd class\n",
    "Rose = np.array([1.0,17.0,1.0,0.0]) # 1st class\n",
    "You = np.array([1.0,48.0,0.0,0.0]) # 2nd class\n",
    "\n",
    "# Combine passenger arrays as an NumPy array\n",
    "sample_passengers = np.array([Jack, Rose, You])\n",
    "\n",
    "# Since our Logistic Regression model was trained on scaled feature data, we must also scale the feature data we are making predictions on.\n",
    "sample_passengers_scaled = scaler.transform(sample_passengers)\n",
    "print(sample_passengers_scaled)\n",
    "# [[-0.71267151 -0.73013432 -0.56813051 -0.51492865]\n",
    "#  [ 1.40317101 -0.95824651  1.76015894 -0.51492865]\n",
    "#  [-0.71267151  1.39891283 -0.56813051  1.94201662]]\n",
    "print('--------------------------------------------')\n",
    "# Make survival predictions!\n",
    "print(model.predict(sample_passengers_scaled))\n",
    "# [0 1 0] => only Rose made it!\n",
    "\n",
    "#  Call your model's .predict_proba() to see the probabilities that led to these predictions. The 1st column is the probability of a passenger perishing on the Titanic, and the 2nd column is the probability of a passenger surviving the sinking\n",
    "print(model.predict_proba(sample_passengers_scaled))\n",
    "# [[0.88988293 0.11011707]\n",
    "#  [0.06308681 0.93691319]\n",
    "#  [0.83347911 0.16652089]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
