{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Validation and Test sets\n",
    "\n",
    "In order to test the effectiveness of your algorithm, we’ll split this data into:\n",
    "\n",
    " - training set\n",
    " - validation set\n",
    " - test set\n",
    " \n",
    "The training set is the data that the algorithm will learn from. \n",
    "\n",
    "After training, the validation set is used to compute the accuracy or error of the algorithm. Although we know the true labels of every point in the validation set, but we pretend we don’t. We can use the validation set to determine the accuracy of our algorithm and calulate the validation error, precision, recall, or F1 score.\n",
    "\n",
    "Determining how to split your data set into and validation can be tricky. If your training set is too small, then your algorithm might not have enough data to effectively learn. On the other hand, if your validation set is too small, then your accuracy, precision, recall, and F1 score could have a large variance. In general an 80%/20%, testing/validation is the starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Fold Cross-Validation\n",
    "\n",
    "Sometimes your dataset is so small, that splitting it 80/20 will still result in a large amount of variance. One solution to this is to perform N-Fold Cross-Validation. The central idea here is that we’re going to do this entire process N times and average the accuracy. For example, in 10-fold cross-validation, we’ll make the validation set the first 10% of the data and calculate accuracy, precision, recall and F1 score. We'll use the other 90% to traing the algorithm.\n",
    "\n",
    "We’ll then repeat the process, making the validation set the second 10% of the data and calculate these statistics again, using the remaining 90% as the training set. We can repeat this process 10 times, and every time the validation set will be a different chunk of the data. If we then average all of the accuracies, we will have a better sense of how our model does on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data set\n",
    "\n",
    "Once you’re happy with your model’s performance, it is time to introduce the test set. This is part of your data that you partitioned away at the very start of your experiment. It’s meant to be a substitute for the data in the real world that you’re actually interested in classifying. It functions very similarly to the validation set, except you never touched this data while building or tuning your model. By finding the accuracy, precision, recall, and F1 score on the test set, you get a good understanding of how well your algorithm will do in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
