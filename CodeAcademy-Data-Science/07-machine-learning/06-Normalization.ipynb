{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "When we try and find trends in data by comparing features of a dataset, problems arise when the two datasets are on different scale. Which will lead to the feature dominating the others, and so losing information. Normalization is an essential part of machine learning.\n",
    "\n",
    "For example, consider a dataset of houses. Two potential features might be the number of rooms in the house, and the total age of the house in years. A machine learning algorithm could try to predict which house would be best for you. However, when the algorithm compares data points, the feature with the larger scale will completely dominate the other. \n",
    "\n",
    "![Normalization](img/normalization-1.png)\n",
    "\n",
    "There is a large difference between a house with 2 rooms and one 20 rooms. But right now, because two houses can be 100 years apart, the difference in the number of rooms contributes less to the overall difference.\n",
    "\n",
    "As a more extreme example would be to plot house price on the x-axis. The difference in the number of rooms would be even less relevant because the cost of two houses could have a difference of thousands of dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of normalization is to make every datapoint have the same scale so each feature is equally important. The image below shows the same house data normalized using min-max normalization.\n",
    "\n",
    "![Normalization](img/normalization-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Normalization\n",
    "\n",
    "Is one of the most common ways to normalize data. Guarantees all features will have the exact same scale. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.\n",
    "\n",
    "```py\n",
    "normalized_value = (value - min) / (max - min)\n",
    "```\n",
    "\n",
    "Note:\n",
    "\n",
    "Min-Max algorithm does not handle outliers, e.g. if you have 99 values between 0 and 40, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.4. That data is just as squished as before!\n",
    "\n",
    "![Normalization](img/normalization-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing fixed the squishing problem on the y-axis, but the x-axis is still problematic. Now if we were to compare these points, the y-axis would dominate; the y-axis can differ by 1, but the x-axis can only differ by 0.4.\n",
    "\n",
    "### Z-Score Normalization\n",
    "\n",
    "Avoids the outlier issue, but does not produce normalized data with the exact same scale.\n",
    "\n",
    "```py\n",
    "normalized_value = (value - mean_value_of_feature) / std_deviation_of_feature\n",
    "```\n",
    "\n",
    "If a value is exactly equal to the mean of all the values of the feature, it will be normalized to 0. If it is below the mean, it will be a negative number, and if it is above the mean it will be a positive number. The size of those negative and positive numbers is determined by the standard deviation of the original feature. If the unnormalized data had a large standard deviation, the normalized values will be closer to 0.\n",
    "\n",
    "![Normalization](img/normalization-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the data still looks squished, notice that the points are now on roughly the same scale for both features â€” almost all points are between -2 and 2 on both the x-axis and y-axis. The only potential downside is that the features aren't on the exact same scale.\n",
    "\n",
    "With min-max normalization, we were guaranteed to reshape both of our features to be between 0 and 1. Using z-score normalization, the x-axis now has a range from about -1.5 to 1.5 while the y-axis has a range from about -2 to 2. This is certainly better than before; the x-axis, which previously had a range of 0 to 40, is no longer dominating the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
